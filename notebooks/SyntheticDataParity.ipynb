{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA35PdBcea3w"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUrk1PVVxyqC",
        "outputId": "7246ebcf-b295-41e6-df36-623e0fa04203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.32.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOeyKdqJefhP"
      },
      "source": [
        "# Data Manager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t4FOIcHKwxGY"
      },
      "outputs": [],
      "source": [
        "from yaml import safe_load\n",
        "from typing import Union, Optional, List\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from json import load as json_load\n",
        "\n",
        "class ConfigLoader:\n",
        "    \"\"\"\n",
        "    Handles loading configuration files for data analysis.\n",
        "    Supports YAML and JSON formats.\n",
        "\n",
        "    Reads configuration data from a specified file path and provides a dictionary-like interface to access configuration values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.config: Optional[dict] = None\n",
        "        self.file_path: Optional[Union[str, Path]] = None\n",
        "        self.file_type: Optional[str] = None\n",
        "\n",
        "    def load(self, file_path: Union[str, Path]) -> None:\n",
        "        \"\"\"\n",
        "        Load configuration from a file.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str or Path\n",
        "            Path to the configuration file (YAML or JSON).\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the file format is unsupported or if the file cannot be read.\n",
        "        \"\"\"\n",
        "        self.file_path = Path(file_path)\n",
        "        ext = self.file_path.suffix.lower()\n",
        "\n",
        "        if ext in ['.yaml', '.yml']:\n",
        "            self.file_type = 'yaml'\n",
        "            with open(self.file_path, 'r') as f:\n",
        "                self.config = safe_load(f)\n",
        "        elif ext in ['.json']:\n",
        "            self.file_type = 'json'\n",
        "            with open(self.file_path, 'r') as f:\n",
        "                self.config = json_load(f)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {ext}. Supported formats are YAML and JSON.\")\n",
        "        if not isinstance(self.config, dict):\n",
        "            raise ValueError(\"Configuration file must contain a valid dictionary.\")\n",
        "        if not self.config:\n",
        "            raise ValueError(\"Configuration file is empty.\")\n",
        "\n",
        "    def get_configs(self, file_path: Optional[Union[str, Path]] = None) -> dict:\n",
        "        \"\"\"\n",
        "        Get the loaded configuration data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str or Path, optional\n",
        "            If provided, will load the configuration from this file instead of the previously loaded one.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            The loaded configuration data.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If no configuration has been loaded.\n",
        "        \"\"\"\n",
        "        if file_path:\n",
        "            self.load(file_path)\n",
        "\n",
        "        if self.config is None:\n",
        "            raise ValueError(\"No configuration has been loaded.\")\n",
        "\n",
        "        return self.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uakwiNOrw-AJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Tuple, Union, Optional\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure the root logger\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"\n",
        "    Loads and manages reference and comparison datasets for analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.df_ref: Optional[pd.DataFrame] = None\n",
        "        self.df_cmp: Optional[pd.DataFrame] = None\n",
        "        self.common_columns: List[str] = []\n",
        "        self.logger.info(\"Initialized DataLoader instance.\")\n",
        "\n",
        "    def load(\n",
        "        self,\n",
        "        ref_path: Union[str, Path],\n",
        "        cmp_path: Union[str, Path],\n",
        "        ref_sheet_name: Optional[str] = None,\n",
        "        cmp_sheet_name: Optional[str] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Load datasets from specified file paths.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ref_path : str or Path\n",
        "            Path to the reference dataset file (CSV, Parquet or Excel).\n",
        "        cmp_path : str or Path\n",
        "            Path to the comparison dataset file (CSV, Parquet or Excel).\n",
        "        ref_sheet_name : str, optional\n",
        "            Name of the sheet to read (for Excel files).\n",
        "        cmp_sheet_name : str, optional\n",
        "            Name of the sheet to read (for Excel files).\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If file extensions are unsupported or if there are no common columns.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Starting load: ref_path={ref_path}, cmp_path={cmp_path}\")\n",
        "\n",
        "        # Read reference file\n",
        "        self.logger.info(f\"Reading reference dataset from: {ref_path}\")\n",
        "        self.df_ref = self._read_file(ref_path, sheet_name=ref_sheet_name)\n",
        "        if self.df_ref is None:\n",
        "            self.logger.error(f\"Failed to load reference dataset from {ref_path}\")\n",
        "            raise ValueError(f\"Failed to load reference dataset from {ref_path}\")\n",
        "        if not isinstance(self.df_ref, pd.DataFrame):\n",
        "            self.logger.error(f\"Reference dataset at {ref_path} is not a valid DataFrame.\")\n",
        "            raise ValueError(f\"Reference dataset at {ref_path} is not a valid DataFrame.\")\n",
        "        self.logger.info(f\"Successfully loaded reference dataset ({len(self.df_ref)} rows, {len(self.df_ref.columns)} columns).\")\n",
        "\n",
        "        # Read comparison file\n",
        "        self.logger.info(f\"Reading comparison dataset from: {cmp_path}\")\n",
        "        self.df_cmp = self._read_file(cmp_path, sheet_name=cmp_sheet_name)\n",
        "        if self.df_cmp is None:\n",
        "            self.logger.error(f\"Failed to load comparison dataset from {cmp_path}\")\n",
        "            raise ValueError(f\"Failed to load comparison dataset from {cmp_path}\")\n",
        "        if not isinstance(self.df_cmp, pd.DataFrame):\n",
        "            self.logger.error(f\"Comparison dataset at {cmp_path} is not a valid DataFrame.\")\n",
        "            raise ValueError(f\"Comparison dataset at {cmp_path} is not a valid DataFrame.\")\n",
        "        self.logger.info(f\"Successfully loaded comparison dataset ({len(self.df_cmp)} rows, {len(self.df_cmp.columns)} columns).\")\n",
        "\n",
        "        # Determine common columns\n",
        "        self.common_columns = list(set(self.df_ref.columns).intersection(self.df_cmp.columns))\n",
        "        if not self.common_columns:\n",
        "            self.logger.error(\"No common columns found between reference and comparison datasets.\")\n",
        "            raise ValueError(\"No common columns between reference and comparison datasets.\")\n",
        "\n",
        "        self.logger.info(f\"Common columns ({len(self.common_columns)}): {self.common_columns}\")\n",
        "\n",
        "    def _read_file(\n",
        "        self,\n",
        "        path: Union[str, Path],\n",
        "        sheet_name: Optional[str] = None\n",
        "    ) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Read a DataFrame from a file path, inferring format from the extension.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : str or Path\n",
        "            Path to the dataset file.\n",
        "        sheet_name : str, optional\n",
        "            Name of the sheet to read (for Excel files).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame or None\n",
        "            Loaded DataFrame, or None if reading failed.\n",
        "        \"\"\"\n",
        "        ext = Path(path).suffix.lower()\n",
        "        self.logger.info(f\"Attempting to read file '{path}' with extension '{ext}'\")\n",
        "        try:\n",
        "            if ext == '.csv':\n",
        "                df = pd.read_csv(path)\n",
        "                self.logger.info(f\"Read CSV file: {path}\")\n",
        "                return df\n",
        "            elif ext in ('.parquet', '.parq', '.pq'):\n",
        "                df = pd.read_parquet(path)\n",
        "                self.logger.info(f\"Read Parquet file: {path}\")\n",
        "                return df\n",
        "            elif ext in ('.xlsx', '.xls', '.xlsm'):\n",
        "                if sheet_name is None:\n",
        "                    df = pd.read_excel(path)\n",
        "                    self.logger.info(f\"Read Excel file (default sheet): {path}\")\n",
        "                else:\n",
        "                    df = pd.read_excel(path, sheet_name=sheet_name)\n",
        "                    self.logger.info(f\"Read Excel file (sheet='{sheet_name}'): {path}\")\n",
        "                return df\n",
        "            else:\n",
        "                error_msg = f\"Unsupported file format: {ext}\"\n",
        "                self.logger.error(error_msg)\n",
        "                raise ValueError(error_msg)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error reading file '{path}': {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_data(\n",
        "        self,\n",
        "        ref_path: Optional[Union[str, Path]] = None,\n",
        "        cmp_path: Optional[Union[str, Path]] = None,\n",
        "        ref_sheet_name: Optional[str] = None,\n",
        "        cmp_sheet_name: Optional[str] = None\n",
        "    ) -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
        "        \"\"\"\n",
        "        Return the loaded reference and comparison DataFrames (and common columns).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ref_path : str or Path, optional\n",
        "            Path to the reference dataset file (if not already loaded).\n",
        "        cmp_path : str or Path, optional\n",
        "            Path to the comparison dataset file (if not already loaded).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple(pd.DataFrame, pd.DataFrame, List[str])\n",
        "            (reference DataFrame, comparison DataFrame, list of common columns)\n",
        "        \"\"\"\n",
        "        if ref_path and cmp_path:\n",
        "            self.logger.info(\"Paths provided to get_data; invoking load()\")\n",
        "            self.load(\n",
        "                ref_path=ref_path,\n",
        "                cmp_path=cmp_path,\n",
        "                ref_sheet_name=ref_sheet_name,\n",
        "                cmp_sheet_name=cmp_sheet_name\n",
        "            )\n",
        "        if self.df_ref is None:\n",
        "            self.logger.error(\"Reference dataset is not loaded.\")\n",
        "            raise ValueError(\"Reference dataset is not loaded.\")\n",
        "        if self.df_cmp is None:\n",
        "            self.logger.error(\"Comparison dataset is not loaded.\")\n",
        "            raise ValueError(\"Comparison dataset is not loaded.\")\n",
        "        if not self.common_columns:\n",
        "            self.logger.error(\"No common columns identified; cannot proceed.\")\n",
        "            raise ValueError(\"No common columns between reference and comparison datasets.\")\n",
        "\n",
        "        self.logger.info(\"Returning loaded data and common columns.\")\n",
        "        return self.df_ref, self.df_cmp, self.common_columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataFrame Schema Enforcer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import datetime\n",
        "import logging\n",
        "from decimal import Decimal\n",
        "from dateutil.parser import parse as parse_date\n",
        "\n",
        "# Configure the root logger\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "\n",
        "class DataFrameSchemaEnforcer:\n",
        "    \"\"\"\n",
        "    Enforces column types on a DataFrame based on a user-provided schema.\n",
        "\n",
        "    schema: dict mapping column names to target types, where target types can be:\n",
        "      - Python types: int, float, str, bool, Decimal, datetime.datetime\n",
        "      - Strings: 'int', 'float', 'string', 'boolean', 'datetime', 'decimal', 'category'\n",
        "    \"\"\"\n",
        "    _NUM_RE = re.compile(r\"^\\s*([+-]?\\d+(?:\\.\\d+)?)([kmbtKMBT])?\\s*$\")\n",
        "    _SUFFIX_FACTORS = {\n",
        "        'k': 1e3,\n",
        "        'm': 1e6,\n",
        "        'b': 1e9,\n",
        "        't': 1e12,\n",
        "    }\n",
        "\n",
        "    def __init__(self, schema: dict):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.schema = schema\n",
        "        self.handlers = {\n",
        "            int: self._handle_int,\n",
        "            'int': self._handle_int,\n",
        "            float: self._handle_float,\n",
        "            'float': self._handle_float,\n",
        "            str: self._handle_str,\n",
        "            'string': self._handle_str,\n",
        "            'text': self._handle_str,\n",
        "            bool: self._handle_bool,\n",
        "            'boolean': self._handle_bool,\n",
        "            datetime.datetime: self._handle_datetime,\n",
        "            'datetime': self._handle_datetime,\n",
        "            Decimal: self._handle_decimal,\n",
        "            'decimal': self._handle_decimal,\n",
        "            'category': self._handle_category,\n",
        "        }\n",
        "        self.logger.info(f\"Initialized DataFrameSchemaEnforcer with schema: {self.schema}\")\n",
        "\n",
        "    def enforce(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Return a new DataFrame with columns cast according to the schema.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Starting schema enforcement on DataFrame.\")\n",
        "        result = df.copy()\n",
        "        for col, tgt in self.schema.items():\n",
        "            if col not in result.columns:\n",
        "                self.logger.warning(f\"Column '{col}' not found in DataFrame; skipping.\")\n",
        "                continue\n",
        "\n",
        "            handler = self.handlers.get(tgt)\n",
        "            if handler is None:\n",
        "                self.logger.warning(f\"No handler for target type '{tgt}' on column '{col}'; skipping.\")\n",
        "                continue\n",
        "\n",
        "            self.logger.info(f\"Enforcing type '{tgt}' on column '{col}'.\")\n",
        "            try:\n",
        "                result[col] = handler(result[col])\n",
        "                self.logger.info(f\"Column '{col}' successfully cast to '{tgt}'.\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Failed to cast column '{col}' to '{tgt}': {e}\")\n",
        "                raise\n",
        "\n",
        "        self.logger.info(\"Schema enforcement complete.\")\n",
        "        return result\n",
        "\n",
        "    def _handle_numeric(self, series: pd.Series) -> pd.Series:\n",
        "        \"\"\"Parse numeric strings with commas and suffixes to floats.\"\"\"\n",
        "        self.logger.info(\"Parsing numeric values.\")\n",
        "        def parse(val):\n",
        "            if pd.isna(val):\n",
        "                return np.nan\n",
        "            s = str(val).replace(',', '').strip()\n",
        "            m = self._NUM_RE.match(s)\n",
        "            if m:\n",
        "                num = float(m.group(1))\n",
        "                suf = m.group(2)\n",
        "                if suf:\n",
        "                    num *= self._SUFFIX_FACTORS[suf.lower()]\n",
        "                return num\n",
        "            try:\n",
        "                return float(s)\n",
        "            except Exception:\n",
        "                return np.nan\n",
        "\n",
        "        parsed_series = series.map(parse)\n",
        "        self.logger.info(\"Numeric parsing complete.\")\n",
        "        return parsed_series\n",
        "\n",
        "    def _handle_int(self, series: pd.Series) -> pd.Series:\n",
        "        self.logger.info(\"Casting series to integer ('Int64').\")\n",
        "        floats = self._handle_numeric(series)\n",
        "        try:\n",
        "            int_series = floats.round(0).astype('Int64')\n",
        "            self.logger.info(\"Integer casting complete.\")\n",
        "            return int_series\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error casting to integer: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _handle_float(self, series: pd.Series) -> pd.Series:\n",
        "        self.logger.info(\"Casting series to float.\")\n",
        "        try:\n",
        "            float_series = self._handle_numeric(series).astype(float)\n",
        "            self.logger.info(\"Float casting complete.\")\n",
        "            return float_series\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error casting to float: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _handle_str(self, series: pd.Series) -> pd.Series:\n",
        "        self.logger.info(\"Casting series to string.\")\n",
        "        try:\n",
        "            str_series = series.astype(str)\n",
        "            self.logger.info(\"String casting complete.\")\n",
        "            return str_series\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error casting to string: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _handle_bool(self, series: pd.Series) -> pd.Series:\n",
        "        self.logger.info(\"Casting series to boolean.\")\n",
        "        def parse(val):\n",
        "            if pd.isna(val):\n",
        "                return pd.NA\n",
        "            s = str(val).strip().lower()\n",
        "            if s in ('true','1','yes','y','t'):\n",
        "                return True\n",
        "            if s in ('false','0','no','n','f'):\n",
        "                return False\n",
        "            return pd.NA\n",
        "\n",
        "        try:\n",
        "            bool_series = series.map(parse).astype('boolean')\n",
        "            self.logger.info(\"Boolean casting complete.\")\n",
        "            return bool_series\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error casting to boolean: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _handle_datetime(self, series: pd.Series) -> pd.Series:\n",
        "        self.logger.info(\"Casting series to datetime.\")\n",
        "        try:\n",
        "            dt_series = pd.to_datetime(series, errors='coerce', infer_datetime_format=True)\n",
        "            self.logger.info(\"Datetime casting complete.\")\n",
        "            return dt_series\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error casting to datetime: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _handle_decimal(self, series: pd.Series) -> pd.Series:\n",
        "        self.logger.info(\"Casting series to Decimal.\")\n",
        "        def parse(val):\n",
        "            if pd.isna(val):\n",
        "                return None\n",
        "            s = str(val).replace(',', '').strip()\n",
        "            try:\n",
        "                return Decimal(s)\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "        try:\n",
        "            dec_series = series.map(parse)\n",
        "            self.logger.info(\"Decimal casting complete.\")\n",
        "            return dec_series\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error casting to Decimal: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _handle_category(self, series: pd.Series) -> pd.Series:\n",
        "        self.logger.info(\"Casting series to category.\")\n",
        "        try:\n",
        "            cat_series = series.astype('category')\n",
        "            self.logger.info(\"Category casting complete.\")\n",
        "            return cat_series\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error casting to category: {e}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z3e5EaPCw_1_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "# from src.data_management.config_loader import ConfigLoader\n",
        "# from src.data_management.data_loader import DataLoader\n",
        "# from src.data_management.schema_enforcer import DataFrameSchemaEnforcer\n",
        "\n",
        "# Configure the root logger\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "\n",
        "class DataManager:\n",
        "    \"\"\"\n",
        "    Manages data loading and configuration for data analysis.\n",
        "\n",
        "    This class integrates the functionality of loading configuration files and datasets,\n",
        "    providing a unified interface for data management tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.logger.info(\"Initializing DataManager.\")\n",
        "        self.config_loader = ConfigLoader()\n",
        "        self.data_loader = DataLoader()\n",
        "\n",
        "    def load_config(self, file_path: str) -> dict:\n",
        "        \"\"\"\n",
        "        Load configuration from a specified file path.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str\n",
        "            Path to the configuration file (YAML or JSON).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            Loaded configuration data.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Loading configuration from: {file_path}\")\n",
        "        try:\n",
        "            self.config_loader.get_configs(file_path)\n",
        "            config = self.config_loader.config\n",
        "            self.logger.info(f\"Configuration loaded successfully from: {file_path}\")\n",
        "            return config\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load configuration from {file_path}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def load_data(\n",
        "        self,\n",
        "        ref_path: str,\n",
        "        cmp_path: str,\n",
        "        ref_sheet_name: str = None,\n",
        "        cmp_sheet_name: str = None\n",
        "    ) -> tuple[pd.DataFrame, pd.DataFrame, list[str]]:\n",
        "        \"\"\"\n",
        "        Load datasets from specified file paths.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ref_path : str\n",
        "            Path to the reference dataset file (CSV, Parquet or Excel).\n",
        "        cmp_path : str\n",
        "            Path to the comparison dataset file (CSV, Parquet or Excel).\n",
        "        ref_sheet_name : str, optional\n",
        "            Name of the sheet to read for the reference dataset (for Excel files).\n",
        "        cmp_sheet_name : str, optional\n",
        "            Name of the sheet to read for the comparison dataset (for Excel files).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple[pd.DataFrame, pd.DataFrame, list[str]]\n",
        "            Reference dataset, comparison dataset, and list of common columns.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If file extensions are unsupported or if there are no common columns.\n",
        "        \"\"\"\n",
        "        self.logger.info(\n",
        "            f\"Loading data: ref_path={ref_path}, cmp_path={cmp_path}, \"\n",
        "            f\"ref_sheet_name={ref_sheet_name}, cmp_sheet_name={cmp_sheet_name}\"\n",
        "        )\n",
        "        try:\n",
        "            df_ref, df_cmp, common_columns = self.data_loader.get_data(\n",
        "                ref_path, cmp_path, ref_sheet_name, cmp_sheet_name\n",
        "            )\n",
        "            self.logger.info(\n",
        "                f\"Data loaded successfully: \"\n",
        "                f\"ref_rows={len(df_ref)}, ref_cols={len(df_ref.columns)}; \"\n",
        "                f\"cmp_rows={len(df_cmp)}, cmp_cols={len(df_cmp.columns)}; \"\n",
        "                f\"common_columns={common_columns}\"\n",
        "            )\n",
        "            return df_ref, df_cmp, common_columns\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def enforce_schema(self, df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Enforce a schema on a DataFrame.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame to enforce the schema on.\n",
        "        schema : dict\n",
        "            Schema definition to enforce.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame\n",
        "            DataFrame with enforced schema.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the schema enforcement fails.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Enforcing schema on DataFrame.\")\n",
        "        try:\n",
        "            enforcer = DataFrameSchemaEnforcer(schema)\n",
        "            df_enforced = enforcer.enforce(df)\n",
        "            self.logger.info(\"Schema enforcement successful.\")\n",
        "            return df_enforced\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Schema enforcement failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def pre_process_data(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        schema: dict\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Pre-process a DataFrame by enforcing a schema.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame to pre-process.\n",
        "        schema : dict\n",
        "            Schema definition to enforce.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame\n",
        "            Pre-processed DataFrame with enforced schema.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the pre-processing fails.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Pre-processing DataFrame.\")\n",
        "        try:\n",
        "            df_preprocessed = self.enforce_schema(df, schema)\n",
        "            self.logger.info(\"Pre-processing completed successfully.\")\n",
        "            return df_preprocessed\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Pre-processing failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_ref_and_cmp_data(\n",
        "        self,\n",
        "        ref_path: str,\n",
        "        cmp_path: str,\n",
        "        ref_data_schema_path: str,\n",
        "        cmp_data_schema_path: str,\n",
        "        ref_sheet_name: str = None,\n",
        "        cmp_sheet_name: str = None,\n",
        "    ) -> tuple[pd.DataFrame, pd.DataFrame, list[str], dict, dict]:\n",
        "        \"\"\"\n",
        "        Load reference and comparison datasets, enforce schemas, and return them with common columns.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ref_path : str\n",
        "            Path to the reference dataset file.\n",
        "        cmp_path : str\n",
        "            Path to the comparison dataset file.\n",
        "        ref_data_schema_path : str\n",
        "            Path to the reference data schema file (JSON/YAML).\n",
        "        cmp_data_schema_path : str\n",
        "            Path to the comparison data schema file (JSON/YAML).\n",
        "        ref_sheet_name : str, optional\n",
        "            Name of the sheet for the reference dataset (if applicable).\n",
        "        cmp_sheet_name : str, optional\n",
        "            Name of the sheet for the comparison dataset (if applicable).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple[pd.DataFrame, pd.DataFrame, list[str], dict, dict]\n",
        "            Tuple containing:\n",
        "            - Reference DataFrame (schema enforced)\n",
        "            - Comparison DataFrame (schema enforced)\n",
        "            - List of common columns\n",
        "            - Reference schema dict\n",
        "            - Comparison schema dict\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Starting full data retrieval and schema enforcement process.\")\n",
        "        try:\n",
        "            # Load raw data\n",
        "            df_ref, df_cmp, common_columns = self.load_data(\n",
        "                ref_path, cmp_path, ref_sheet_name, cmp_sheet_name\n",
        "            )\n",
        "\n",
        "            # Load schemas\n",
        "            self.logger.info(f\"Loading reference schema from: {ref_data_schema_path}\")\n",
        "            ref_schema = self.load_config(ref_data_schema_path)\n",
        "            self.logger.info(f\"Loading comparison schema from: {cmp_data_schema_path}\")\n",
        "            cmp_schema = self.load_config(cmp_data_schema_path)\n",
        "\n",
        "            # Enforce schemas\n",
        "            self.logger.info(\"Enforcing schema on reference DataFrame.\")\n",
        "            df_ref_enforced = self.pre_process_data(df_ref, ref_schema)\n",
        "            self.logger.info(\"Enforcing schema on comparison DataFrame.\")\n",
        "            df_cmp_enforced = self.pre_process_data(df_cmp, cmp_schema)\n",
        "\n",
        "            self.logger.info(\n",
        "                f\"Finished processing reference and comparison data. \"\n",
        "                f\"Returning data with {len(common_columns)} common columns.\"\n",
        "            )\n",
        "            return df_ref_enforced, df_cmp_enforced, common_columns, ref_schema, cmp_schema\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"get_ref_and_cmp_data failed: {e}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqiSBblEsF2X"
      },
      "source": [
        "## Version 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "from scipy.stats import gaussian_kde, entropy\n",
        "from scipy.stats import ks_2samp, wasserstein_distance\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from abc import ABC, abstractmethod\n",
        "import gradio as gr\n",
        "import logging\n",
        "\n",
        "# ─── Logging Configuration ────────────────────────────────────────────────────\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Make sure DataManager is importable. If running in Colab, you can either just put the DataManager code in a cell above or do:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# import sys\n",
        "# sys.path.append('/content/drive/MyDrive/your_project_folder')\n",
        "# from data_manager import DataManager\n",
        "\n",
        "from src.data_management.data_manager import DataManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Report Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Report:\n",
        "    def __init__(\n",
        "        self,\n",
        "        ref_df: pd.DataFrame,\n",
        "        cmp_df: pd.DataFrame,\n",
        "        common_columns: list,\n",
        "        ref_schema: dict,\n",
        "        cmp_schema: dict\n",
        "    ):\n",
        "        logger.info(\"Initializing Report object.\")\n",
        "        self.ref_df = ref_df.loc[:, common_columns].reset_index(drop=True)\n",
        "        self.cmp_df = cmp_df.loc[:, common_columns].reset_index(drop=True)\n",
        "        self.common_columns = common_columns\n",
        "        self.ref_schema = ref_schema\n",
        "        self.cmp_schema = cmp_schema\n",
        "\n",
        "        logger.info(f\"Common columns: {common_columns}\")\n",
        "        # Determine effective column types using schema + heuristics\n",
        "        self.column_types = {}\n",
        "        for col in common_columns:\n",
        "            declared = ref_schema.get(col)\n",
        "            logger.info(f\"Determining type for column '{col}' (declared: {declared}).\")\n",
        "            if declared == 'category':\n",
        "                self.column_types[col] = 'categorical'\n",
        "            elif declared in (bool, 'boolean'):\n",
        "                self.column_types[col] = 'categorical'\n",
        "            elif declared in (int, 'int', float, 'float', 'decimal'):\n",
        "                self.column_types[col] = 'numeric'\n",
        "            elif declared in (pd.Timestamp, 'datetime', 'datetime64[ns]'):\n",
        "                self.column_types[col] = 'datetime'\n",
        "            elif declared in (str, 'string', 'text'):\n",
        "                non_na = self.ref_df[col].dropna().astype(str)\n",
        "                if len(non_na) == 0:\n",
        "                    self.column_types[col] = 'categorical'\n",
        "                else:\n",
        "                    unique_ratio = non_na.nunique() / len(non_na)\n",
        "                    mean_tokens = non_na.str.split().apply(len).mean()\n",
        "                    if unique_ratio <= 0.20 and mean_tokens <= 5:\n",
        "                        self.column_types[col] = 'categorical'\n",
        "                    else:\n",
        "                        self.column_types[col] = 'text'\n",
        "            else:\n",
        "                dtype = self.ref_df[col].dtype\n",
        "                if pd.api.types.is_numeric_dtype(dtype):\n",
        "                    self.column_types[col] = 'numeric'\n",
        "                elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
        "                    self.column_types[col] = 'datetime'\n",
        "                else:\n",
        "                    self.column_types[col] = 'categorical'\n",
        "            logger.info(f\"  → Column '{col}' determined to be '{self.column_types[col]}'.\")\n",
        "\n",
        "        # Placeholder for per‐column metrics\n",
        "        self.per_column_metrics = {col: {} for col in common_columns}\n",
        "        logger.info(\"Report initialization complete.\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "        logger.info(\"Computing Jensen–Shannon divergence.\")\n",
        "        p = p + 1e-12\n",
        "        q = q + 1e-12\n",
        "        p = p / p.sum()\n",
        "        q = q / q.sum()\n",
        "        m = 0.5 * (p + q)\n",
        "        result = 0.5 * entropy(p, m, base=2) + 0.5 * entropy(q, m, base=2)\n",
        "        logger.info(f\"  → JS divergence = {result:.6f}\")\n",
        "        return result\n",
        "\n",
        "\n",
        "    def numeric_ks_similarity(self, col: str) -> float:\n",
        "        logger.info(f\"Computing KS similarity for numeric column '{col}'.\")\n",
        "        real_vals = self.ref_df[col].dropna().astype(float).values\n",
        "        syn_vals  = self.cmp_df[col].dropna().astype(float).values\n",
        "        logger.info(f\"  → Samples: real={real_vals.size}, syn={syn_vals.size}\")\n",
        "        if real_vals.size < 2 or syn_vals.size < 2:\n",
        "            logger.info(\"  → Not enough data points for KS: returning NaN.\")\n",
        "            return np.nan\n",
        "        D_stat, _ = ks_2samp(real_vals, syn_vals)\n",
        "        sim = 1.0 - D_stat\n",
        "        logger.info(f\"  → KS D-statistic = {D_stat:.6f}, similarity = {sim:.6f}\")\n",
        "        return sim\n",
        "\n",
        "\n",
        "    def numeric_wasserstein(self, col: str) -> float:\n",
        "        logger.info(f\"Computing Wasserstein distance for numeric column '{col}'.\")\n",
        "        real_vals = self.ref_df[col].dropna().astype(float).values\n",
        "        syn_vals  = self.cmp_df[col].dropna().astype(float).values\n",
        "        logger.info(f\"  → Samples: real={real_vals.size}, syn={syn_vals.size}\")\n",
        "        if real_vals.size < 2 or syn_vals.size < 2:\n",
        "            logger.info(\"  → Not enough data points for Wasserstein: returning NaN.\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        Wraw = wasserstein_distance(real_vals, syn_vals)\n",
        "        logger.info(f\"  → Raw Wasserstein = {Wraw:.6f}\")\n",
        "        rmin, rmax = real_vals.min(), real_vals.max()\n",
        "        rng = rmax - rmin\n",
        "        if rng == 0:\n",
        "            logger.info(\"  → Real range is zero; normalized Wasserstein is NaN.\")\n",
        "            return Wraw, np.nan\n",
        "\n",
        "        Wnorm = Wraw / rng\n",
        "        logger.info(f\"  → Normalized Wasserstein = {Wnorm:.6f}\")\n",
        "        return Wraw, Wnorm\n",
        "\n",
        "\n",
        "    def numeric_summary_stats(self, col: str) -> dict:\n",
        "        logger.info(f\"Computing detailed summary‐stats for numeric column '{col}'.\")\n",
        "        real_vals = self.ref_df[col].dropna().astype(float)\n",
        "        syn_vals  = self.cmp_df[col].dropna().astype(float)\n",
        "        if real_vals.empty or syn_vals.empty:\n",
        "            logger.info(\"  → One side is empty: returning all NaN stats.\")\n",
        "            return {\n",
        "                'mean_real': np.nan, 'mean_syn': np.nan, 'mean_diff': np.nan, 'mean_pct_of_range': np.nan,\n",
        "                'median_real': np.nan, 'median_syn': np.nan, 'median_diff': np.nan, 'median_pct_of_range': np.nan,\n",
        "                'std_real': np.nan, 'std_syn': np.nan, 'std_diff': np.nan, 'std_pct_of_std': np.nan\n",
        "            }\n",
        "\n",
        "        rmin, rmax = real_vals.min(), real_vals.max()\n",
        "        data_range = rmax - rmin\n",
        "        logger.info(f\"  → Real min={rmin:.4f}, max={rmax:.4f}, range={data_range:.4f}\")\n",
        "\n",
        "        mean_r = real_vals.mean()\n",
        "        mean_s = syn_vals.mean()\n",
        "        diff_mean = mean_s - mean_r\n",
        "        pct_mean = diff_mean / data_range if data_range != 0 else np.nan\n",
        "        logger.info(f\"  → Mean: real={mean_r:.4f}, syn={mean_s:.4f}, diff={diff_mean:.4f}, pct_range={pct_mean:.6f}\")\n",
        "\n",
        "        med_r = real_vals.median()\n",
        "        med_s = syn_vals.median()\n",
        "        diff_med = med_s - med_r\n",
        "        pct_med = diff_med / data_range if data_range != 0 else np.nan\n",
        "        logger.info(f\"  → Median: real={med_r:.4f}, syn={med_s:.4f}, diff={diff_med:.4f}, pct_range={pct_med:.6f}\")\n",
        "\n",
        "        std_r = real_vals.std(ddof=1)\n",
        "        std_s = syn_vals.std(ddof=1)\n",
        "        diff_std = std_s - std_r\n",
        "        pct_std = diff_std / std_r if std_r != 0 else np.nan\n",
        "        logger.info(f\"  → StdDev: real={std_r:.4f}, syn={std_s:.4f}, diff={diff_std:.4f}, pct_std={pct_std:.6f}\")\n",
        "\n",
        "        return {\n",
        "            'mean_real': mean_r,\n",
        "            'mean_syn': mean_s,\n",
        "            'mean_diff': diff_mean,\n",
        "            'mean_pct_of_range': pct_mean,\n",
        "            'median_real': med_r,\n",
        "            'median_syn': med_s,\n",
        "            'median_diff': diff_med,\n",
        "            'median_pct_of_range': pct_med,\n",
        "            'std_real': std_r,\n",
        "            'std_syn': std_s,\n",
        "            'std_diff': diff_std,\n",
        "            'std_pct_of_std': pct_std\n",
        "        }\n",
        "\n",
        "\n",
        "    def numeric_range_coverage(self, col: str) -> float:\n",
        "        logger.info(f\"Computing range coverage for numeric column '{col}'.\")\n",
        "        real_vals = self.ref_df[col].dropna().astype(float)\n",
        "        syn_vals  = self.cmp_df[col].dropna().astype(float)\n",
        "        if real_vals.empty or syn_vals.empty:\n",
        "            logger.info(\"  → One side is empty: returning NaN.\")\n",
        "            return np.nan\n",
        "        rmin, rmax = real_vals.min(), real_vals.max()\n",
        "        rng = rmax - rmin\n",
        "        if rng == 0:\n",
        "            logger.info(\"  → Real range is zero: returning NaN.\")\n",
        "            return np.nan\n",
        "        smin, smax = syn_vals.min(), syn_vals.max()\n",
        "        srng = smax - smin\n",
        "        cov = min(srng / rng, 1.0)\n",
        "        logger.info(f\"  → Synthetic range={srng:.4f}, coverage={cov:.6f}\")\n",
        "        return cov\n",
        "\n",
        "\n",
        "    # ───── Univariate Methods (unchanged from earlier) ──────────────────────────\n",
        "\n",
        "    def _compute_numeric_histogram_js(self, col: str) -> float:\n",
        "        logger.info(f\"Computing histogram‐based JS divergence for '{col}'.\")\n",
        "        ref_vals = self.ref_df[col].dropna().astype(float).values\n",
        "        cmp_vals = self.cmp_df[col].dropna().astype(float).values\n",
        "        if len(ref_vals) < 2 or len(cmp_vals) < 2:\n",
        "            logger.info(\"  → Not enough data for histogram JS: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        n_ref = len(ref_vals)\n",
        "        s_ref = np.std(ref_vals, ddof=1)\n",
        "        h = 3.49 * s_ref * (n_ref ** (-1 / 3))\n",
        "        if h <= 0:\n",
        "            k = int(np.ceil(np.log2(n_ref) + 1))\n",
        "        else:\n",
        "            data_min = min(ref_vals.min(), cmp_vals.min())\n",
        "            data_max = max(ref_vals.max(), cmp_vals.max())\n",
        "            k = int(np.ceil((data_max - data_min) / h))\n",
        "            if k < 2:\n",
        "                k = int(np.ceil(np.log2(n_ref) + 1))\n",
        "        k = max(min(k, 100), 2)\n",
        "        logger.info(f\"  → Number of bins for histogram: k={k}\")\n",
        "\n",
        "        data_min = min(ref_vals.min(), cmp_vals.min())\n",
        "        data_max = max(ref_vals.max(), cmp_vals.max())\n",
        "        bins = np.linspace(data_min, data_max, k + 1)\n",
        "\n",
        "        p_ref, _ = np.histogram(ref_vals, bins=bins, density=True)\n",
        "        p_cmp, _ = np.histogram(cmp_vals, bins=bins, density=True)\n",
        "        bin_width = bins[1] - bins[0]\n",
        "        p_ref_probs = p_ref * bin_width\n",
        "        p_cmp_probs = p_cmp * bin_width\n",
        "\n",
        "        js = Report._js_divergence(p_ref_probs, p_cmp_probs)\n",
        "        return js\n",
        "\n",
        "\n",
        "    def _compute_numeric_kde_metrics(self, col: str) -> (float, float):\n",
        "        logger.info(f\"Computing KDE‐based L2 & JS metrics for '{col}'.\")\n",
        "        ref_vals = self.ref_df[col].dropna().astype(float).values\n",
        "        cmp_vals = self.cmp_df[col].dropna().astype(float).values\n",
        "        if len(ref_vals) < 2 or len(cmp_vals) < 2:\n",
        "            logger.info(\"  → Not enough data for KDE: returning (NaN, NaN).\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        pooled = np.concatenate([ref_vals, cmp_vals])\n",
        "        bw = np.std(pooled, ddof=1) * (len(pooled) ** (-1/5))\n",
        "        if bw <= 0:\n",
        "            bw = 1.0\n",
        "        logger.info(f\"  → Bandwidth for KDE (Silverman estimate) = {bw:.6f}\")\n",
        "\n",
        "        try:\n",
        "            kde_ref = gaussian_kde(ref_vals, bw_method=bw / np.std(ref_vals, ddof=1))\n",
        "            kde_cmp = gaussian_kde(cmp_vals, bw_method=bw / np.std(cmp_vals, ddof=1))\n",
        "        except np.linalg.LinAlgError:\n",
        "            logger.info(\"  → KDE failed (singular covariance): returning (NaN, NaN).\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        data_min = pooled.min()\n",
        "        data_max = pooled.max()\n",
        "        grid = np.linspace(data_min, data_max, 512)\n",
        "        pdf_ref = kde_ref(grid)\n",
        "        pdf_cmp = kde_cmp(grid)\n",
        "\n",
        "        dx = grid[1] - grid[0]\n",
        "        pdf_ref_norm = pdf_ref / (np.trapz(pdf_ref, grid))\n",
        "        pdf_cmp_norm = pdf_cmp / (np.trapz(pdf_cmp, grid))\n",
        "\n",
        "        l2 = np.sqrt(np.trapz((pdf_ref_norm - pdf_cmp_norm) ** 2, grid))\n",
        "        logger.info(f\"  → KDE L2 distance = {l2:.6f}\")\n",
        "\n",
        "        p = pdf_ref_norm * dx\n",
        "        q = pdf_cmp_norm * dx\n",
        "        js = Report._js_divergence(p, q)\n",
        "        return l2, js\n",
        "\n",
        "\n",
        "    def categorical_tvd_similarity(self, col: str) -> float:\n",
        "        logger.info(f\"Computing TVD similarity for categorical column '{col}'.\")\n",
        "        real_counts = self.ref_df[col].dropna().value_counts(normalize=True)\n",
        "        syn_counts  = self.cmp_df[col].dropna().value_counts(normalize=True)\n",
        "        all_cats = set(real_counts.index).union(syn_counts.index)\n",
        "        tvd = 0.0\n",
        "        for cat in all_cats:\n",
        "            p = real_counts.get(cat, 0.0)\n",
        "            q = syn_counts.get(cat, 0.0)\n",
        "            tvd += abs(p - q)\n",
        "        tvd *= 0.5\n",
        "        sim = 1.0 - tvd\n",
        "        logger.info(f\"  → TVD = {tvd:.6f}, similarity = {sim:.6f}\")\n",
        "        return sim\n",
        "\n",
        "\n",
        "    def categorical_coverage(self, col: str) -> float:\n",
        "        logger.info(f\"Computing category coverage for '{col}'.\")\n",
        "        real_uniques = set(self.ref_df[col].dropna().unique())\n",
        "        syn_uniques  = set(self.cmp_df[col].dropna().unique())\n",
        "        if not real_uniques:\n",
        "            logger.info(\"  → No real categories: returning NaN.\")\n",
        "            return np.nan\n",
        "        covered = len(real_uniques.intersection(syn_uniques))\n",
        "        coverage = covered / len(real_uniques)\n",
        "        logger.info(f\"  → Coverage = {coverage:.6f} ({covered}/{len(real_uniques)})\")\n",
        "        return coverage\n",
        "\n",
        "\n",
        "    def _compute_categorical_js(self, col: str) -> float:\n",
        "        logger.info(f\"Computing JS divergence on categorical frequencies for '{col}'.\")\n",
        "        ref_counts = self.ref_df[col].dropna().value_counts(normalize=True)\n",
        "        cmp_counts = self.cmp_df[col].dropna().value_counts(normalize=True)\n",
        "        all_cats = sorted(set(ref_counts.index).union(cmp_counts.index))\n",
        "        p = np.array([ref_counts.get(cat, 0.0) for cat in all_cats], dtype=float)\n",
        "        q = np.array([cmp_counts.get(cat, 0.0) for cat in all_cats], dtype=float)\n",
        "        js = Report._js_divergence(p, q)\n",
        "        return js\n",
        "\n",
        "\n",
        "    def _compute_text_token_js(self, col: str, top_k: int = 5000) -> float:\n",
        "        logger.info(f\"Computing token‐level JS divergence for text column '{col}'.\")\n",
        "        texts_ref = self.ref_df[col].dropna().astype(str).tolist()\n",
        "        texts_cmp = self.cmp_df[col].dropna().astype(str).tolist()\n",
        "        if not texts_ref or not texts_cmp:\n",
        "            logger.info(\"  → One side is empty: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        tokens_ref = [tok for t in texts_ref for tok in t.split()]\n",
        "        tokens_cmp = [tok for t in texts_cmp for tok in t.split()]\n",
        "        ctr_ref = Counter(tokens_ref)\n",
        "        ctr_cmp = Counter(tokens_cmp)\n",
        "        combined = ctr_ref + ctr_cmp\n",
        "        most_common = [tok for tok, _ in combined.most_common(top_k)]\n",
        "\n",
        "        p = np.array([ctr_ref.get(tok, 0) for tok in most_common], dtype=float)\n",
        "        q = np.array([ctr_cmp.get(tok, 0) for tok in most_common], dtype=float)\n",
        "        if p.sum() == 0 or q.sum() == 0:\n",
        "            logger.info(\"  → Zero counts on one side: returning NaN.\")\n",
        "            return np.nan\n",
        "        p = p / p.sum()\n",
        "        q = q / q.sum()\n",
        "        js = Report._js_divergence(p, q)\n",
        "        return js\n",
        "\n",
        "\n",
        "    def _compute_text_tfidf_cosine(self, col: str, max_features: int = 5000) -> float:\n",
        "        logger.info(f\"Computing TF–IDF cosine for text column '{col}'.\")\n",
        "        texts_ref = self.ref_df[col].dropna().astype(str).tolist()\n",
        "        texts_cmp = self.cmp_df[col].dropna().astype(str).tolist()\n",
        "        if not texts_ref or not texts_cmp:\n",
        "            logger.info(\"  → One side is empty: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        corpus = texts_ref + texts_cmp\n",
        "        logger.info(f\"  → Fitting TF–IDF on {len(corpus)} documents (max_features={max_features}).\")\n",
        "        vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english', lowercase=True)\n",
        "        tfidf_all = vectorizer.fit_transform(corpus)\n",
        "        n_ref = len(texts_ref)\n",
        "        A_ref = tfidf_all[:n_ref, :]\n",
        "        A_cmp = tfidf_all[n_ref:, :]\n",
        "\n",
        "        c_ref = np.asarray(A_ref.mean(axis=0)).ravel()\n",
        "        c_cmp = np.asarray(A_cmp.mean(axis=0)).ravel()\n",
        "        denom = np.linalg.norm(c_ref) * np.linalg.norm(c_cmp)\n",
        "        cosine = float(np.dot(c_ref, c_cmp) / denom) if denom > 0 else np.nan\n",
        "        logger.info(f\"  → TF–IDF cosine = {cosine:.6f}\")\n",
        "        return cosine\n",
        "\n",
        "\n",
        "    def _compute_text_vocab_jaccard(self, col: str) -> float:\n",
        "        logger.info(f\"Computing vocabulary Jaccard for text column '{col}'.\")\n",
        "        texts_ref = self.ref_df[col].dropna().astype(str).tolist()\n",
        "        texts_cmp = self.cmp_df[col].dropna().astype(str).tolist()\n",
        "        if not texts_ref or not texts_cmp:\n",
        "            logger.info(\"  → One side is empty: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        tokens_ref = set(tok for t in texts_ref for tok in t.split())\n",
        "        tokens_cmp = set(tok for t in texts_cmp for tok in t.split())\n",
        "        if not tokens_ref and not tokens_cmp:\n",
        "            logger.info(\"  → Both sets empty: returning NaN.\")\n",
        "            return np.nan\n",
        "        if not tokens_ref or not tokens_cmp:\n",
        "            logger.info(\"  → One set empty: returning 0.0.\")\n",
        "            return 0.0\n",
        "        inter = tokens_ref.intersection(tokens_cmp)\n",
        "        union = tokens_ref.union(tokens_cmp)\n",
        "        jaccard = len(inter) / len(union)\n",
        "        logger.info(f\"  → Vocabulary Jaccard = {jaccard:.6f}\")\n",
        "        return jaccard\n",
        "\n",
        "\n",
        "    def _compute_document_length_diff(self, col: str) -> float:\n",
        "        logger.info(f\"Computing document‐length difference for text column '{col}'.\")\n",
        "        lens_ref = self.ref_df[col].dropna().astype(str).str.split().apply(len)\n",
        "        lens_cmp = self.cmp_df[col].dropna().astype(str).str.split().apply(len)\n",
        "        if lens_ref.empty or lens_cmp.empty:\n",
        "            logger.info(\"  → One side is empty: returning NaN.\")\n",
        "            return np.nan\n",
        "        diff = abs(lens_ref.mean() - lens_cmp.mean())\n",
        "        logger.info(f\"  → Avg length real = {lens_ref.mean():.2f}, syn = {lens_cmp.mean():.2f}, Δ = {diff:.2f}\")\n",
        "        return diff\n",
        "\n",
        "\n",
        "    def _compute_text_embedding_metrics(\n",
        "        self,\n",
        "        col: str,\n",
        "        max_samples: int = 200\n",
        "    ) -> (float, float):\n",
        "        \"\"\"\n",
        "        Subsample up to max_samples texts from each side,\n",
        "        compute embedding‐cosine and MMD on those.\n",
        "        \"\"\"\n",
        "        logger.info(f\"Computing embedding metrics for text column '{col}' (max_samples={max_samples}).\")\n",
        "        texts_ref = self.ref_df[col].dropna().astype(str).tolist()\n",
        "        texts_cmp = self.cmp_df[col].dropna().astype(str).tolist()\n",
        "        if not texts_ref or not texts_cmp:\n",
        "            logger.info(\"  → One side empty: returning (NaN, NaN).\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        rng = np.random.default_rng(seed=42)\n",
        "        if len(texts_ref) > max_samples:\n",
        "            texts_ref = list(rng.choice(texts_ref, max_samples, replace=False))\n",
        "        if len(texts_cmp) > max_samples:\n",
        "            texts_cmp = list(rng.choice(texts_cmp, max_samples, replace=False))\n",
        "\n",
        "        logger.info(f\"  → Sampling: real={len(texts_ref)}, syn={len(texts_cmp)}.\")\n",
        "        try:\n",
        "            model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "            emb_ref = model.encode(texts_ref, convert_to_numpy=True)\n",
        "            emb_cmp = model.encode(texts_cmp, convert_to_numpy=True)\n",
        "        except Exception as e:\n",
        "            logger.info(f\"  → Error loading/encoding embeddings: {e}. Returning (NaN, NaN).\")\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        cent_ref = emb_ref.mean(axis=0)\n",
        "        cent_cmp = emb_cmp.mean(axis=0)\n",
        "        denom = np.linalg.norm(cent_ref) * np.linalg.norm(cent_cmp)\n",
        "        emb_cosine = float(np.dot(cent_ref, cent_cmp) / denom) if denom > 0 else np.nan\n",
        "        logger.info(f\"  → Embedding cosine = {emb_cosine:.6f}\")\n",
        "\n",
        "        combined = np.vstack([emb_ref, emb_cmp])\n",
        "        pdists = np.sqrt(np.sum((combined[:, None, :] - combined[None, :, :]) ** 2, axis=2))\n",
        "        median_dist = np.median(pdists)\n",
        "        gamma = 1.0 / (2 * (median_dist ** 2 + 1e-12))\n",
        "        logger.info(f\"  → MMD gamma parameter = {gamma:.6e}\")\n",
        "\n",
        "        def rbf_matrix(X, Y, gamma):\n",
        "            dists = np.sum((X[:, None, :] - Y[None, :, :]) ** 2, axis=2)\n",
        "            return np.exp(-gamma * dists)\n",
        "\n",
        "        K_rr = rbf_matrix(emb_ref, emb_ref, gamma)\n",
        "        K_cc = rbf_matrix(emb_cmp, emb_cmp, gamma)\n",
        "        K_rc = rbf_matrix(emb_ref, emb_cmp, gamma)\n",
        "\n",
        "        m = emb_ref.shape[0]\n",
        "        n = emb_cmp.shape[0]\n",
        "        mmd_sq = (np.sum(K_rr) / (m * m)) - (2 * np.sum(K_rc) / (m * n)) + (np.sum(K_cc) / (n * n))\n",
        "        emb_mmd = float(np.sqrt(max(mmd_sq, 0.0)))\n",
        "        logger.info(f\"  → Embedding MMD = {emb_mmd:.6f}\")\n",
        "\n",
        "        return emb_cosine, emb_mmd\n",
        "\n",
        "\n",
        "    def compute_column(self, col: str) -> dict:\n",
        "        \"\"\"\n",
        "        Compute per‐column metrics using the new univariate methods.\n",
        "        \"\"\"\n",
        "        logger.info(f\"compute_column called for '{col}'.\")\n",
        "        results = {}\n",
        "        # Missing-rate difference\n",
        "        mr_diff = abs(self.ref_df[col].isna().mean() - self.cmp_df[col].isna().mean())\n",
        "        results['missing_rate_diff'] = mr_diff\n",
        "        logger.info(f\"  → Missing‐rate difference = {mr_diff:.6f}\")\n",
        "\n",
        "        ctype = self.column_types.get(col)\n",
        "        logger.info(f\"  → Column type = {ctype}\")\n",
        "\n",
        "        if ctype == \"numeric\":\n",
        "            # 1) KS similarity\n",
        "            results['ks_similarity'] = self.numeric_ks_similarity(col)\n",
        "\n",
        "            # 2) Wasserstein (raw + normalized)\n",
        "            Wraw, Wnorm = self.numeric_wasserstein(col)\n",
        "            results['wasserstein_raw']  = Wraw\n",
        "            results['wasserstein_norm'] = Wnorm\n",
        "\n",
        "            # 3) Detailed summary‐stats\n",
        "            stats = self.numeric_summary_stats(col)\n",
        "            results.update(stats)\n",
        "\n",
        "            # 4) Range coverage\n",
        "            results['range_coverage'] = self.numeric_range_coverage(col)\n",
        "\n",
        "        elif ctype == \"categorical\":\n",
        "            # 1) TVD similarity\n",
        "            results['tvd_similarity'] = self.categorical_tvd_similarity(col)\n",
        "            # 2) Category coverage\n",
        "            results['category_coverage'] = self.categorical_coverage(col)\n",
        "\n",
        "        elif ctype == \"text\":\n",
        "            # 1) Token‐JS Divergence\n",
        "            tok_js   = self._compute_text_token_js(col)\n",
        "\n",
        "            # 2) Bigram Jaccard & Bigram‐JS\n",
        "            bigram_j = self._compute_text_bigram_jaccard(col)\n",
        "            bigram_js = self._compute_text_bigram_js(col)\n",
        "\n",
        "            # 3) TF–IDF Cosine\n",
        "            tfidf_cos = self._compute_text_tfidf_cosine(col)\n",
        "\n",
        "            # 4) Vocabulary Jaccard (unigrams)\n",
        "            vocab_j = self._compute_text_vocab_jaccard(col)\n",
        "\n",
        "            # 5) OOV Rate\n",
        "            oov_real = self._compute_text_oov_rate(col)\n",
        "\n",
        "            # 6) Document Length Stats\n",
        "            length_stats = self._compute_text_length_stats(col)\n",
        "            len_real = length_stats['len_real']\n",
        "            len_syn  = length_stats['len_syn']\n",
        "            len_diff = length_stats['len_diff']\n",
        "\n",
        "            # 7) Topic‐Distribution Cosine\n",
        "            topic_cos = self._compute_text_topic_cosine(col)\n",
        "\n",
        "            # 8) Embedding Cosine & MMD\n",
        "            emb_cos, emb_mmd = self._compute_text_embedding_metrics(col)\n",
        "\n",
        "            # Populate results\n",
        "            results['text_tok_js']         = tok_js\n",
        "            results['text_bigram_jaccard'] = bigram_j\n",
        "            results['text_bigram_js']      = bigram_js\n",
        "            results['text_tfidf_cosine']   = tfidf_cos\n",
        "            results['text_vocab_jaccard']  = vocab_j\n",
        "            results['text_oov_rate']       = oov_real\n",
        "            results['len_real']            = len_real\n",
        "            results['len_syn']             = len_syn\n",
        "            results['len_diff']            = len_diff\n",
        "            results['text_topic_cosine']   = topic_cos\n",
        "            results['text_emb_cosine']     = emb_cos\n",
        "            results['text_emb_mmd']        = emb_mmd\n",
        "\n",
        "        else:\n",
        "            # datetime or other types: only missing‐rate\n",
        "            logger.info(f\"  → No specialized univariate for type '{ctype}', returning only missing‐rate.\")\n",
        "\n",
        "        logger.info(f\"  → Completed compute_column for '{col}'.\")\n",
        "        return {'dtype': ctype, **results}\n",
        "\n",
        "\n",
        "    def plot_column_histogram(self, col: str):\n",
        "        logger.info(f\"Generating histogram plot for '{col}'.\")\n",
        "        ref_vals = self.ref_df[col].dropna().astype(float)\n",
        "        cmp_vals = self.cmp_df[col].dropna().astype(float)\n",
        "\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Histogram(\n",
        "            x=ref_vals,\n",
        "            histnorm='probability density',\n",
        "            name='Reference',\n",
        "            opacity=0.6,\n",
        "            nbinsx=30\n",
        "        ))\n",
        "        fig.add_trace(go.Histogram(\n",
        "            x=cmp_vals,\n",
        "            histnorm='probability density',\n",
        "            name='Comparison',\n",
        "            opacity=0.6,\n",
        "            nbinsx=30\n",
        "        ))\n",
        "        fig.update_layout(\n",
        "            barmode='overlay',\n",
        "            title=f\"Histogram (Density) for '{col}'\",\n",
        "            xaxis_title=col,\n",
        "            yaxis_title='Density'\n",
        "        )\n",
        "        logger.info(f\"  → Histogram figure ready for '{col}'.\")\n",
        "        return fig\n",
        "\n",
        "\n",
        "    def plot_column_kde(self, col: str):\n",
        "        logger.info(f\"Generating KDE plot for '{col}'.\")\n",
        "        ref_vals = self.ref_df[col].dropna().astype(float).values\n",
        "        cmp_vals = self.cmp_df[col].dropna().astype(float).values\n",
        "\n",
        "        if len(np.unique(ref_vals)) < 2 or len(np.unique(cmp_vals)) < 2:\n",
        "            logger.info(\"  → Not enough distinct values for KDE: returning empty figure.\")\n",
        "            return go.Figure()\n",
        "\n",
        "        try:\n",
        "            kde_ref = gaussian_kde(ref_vals)\n",
        "            kde_cmp = gaussian_kde(cmp_vals)\n",
        "        except np.linalg.LinAlgError:\n",
        "            logger.info(\"  → KDE failed (singular covariance): returning empty figure.\")\n",
        "            return go.Figure()\n",
        "\n",
        "        pooled = np.concatenate([ref_vals, cmp_vals])\n",
        "        xmin, xmax = pooled.min(), pooled.max()\n",
        "        grid = np.linspace(xmin, xmax, 200)\n",
        "\n",
        "        pdf_ref = kde_ref(grid)\n",
        "        pdf_cmp = kde_cmp(grid)\n",
        "\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Scatter(x=grid, y=pdf_ref, mode='lines', name='Ref KDE'))\n",
        "        fig.add_trace(go.Scatter(x=grid, y=pdf_cmp, mode='lines', name='Cmp KDE'))\n",
        "        fig.update_layout(\n",
        "            title=f\"KDE Plot for '{col}'\",\n",
        "            xaxis_title=col,\n",
        "            yaxis_title='Density'\n",
        "        )\n",
        "        logger.info(f\"  → KDE figure ready for '{col}'.\")\n",
        "        return fig\n",
        "\n",
        "\n",
        "    def plot_category_bar(self, col: str):\n",
        "        logger.info(f\"Generating category bar chart for '{col}'.\")\n",
        "        ref_counts = self.ref_df[col].dropna().value_counts(normalize=True).reset_index()\n",
        "        ref_counts.columns = [col, 'proportion']\n",
        "        ref_counts['dataset'] = 'Reference'\n",
        "\n",
        "        cmp_counts = self.cmp_df[col].dropna().value_counts(normalize=True).reset_index()\n",
        "        cmp_counts.columns = [col, 'proportion']\n",
        "        cmp_counts['dataset'] = 'Comparison'\n",
        "\n",
        "        df_union = pd.concat([ref_counts, cmp_counts], ignore_index=True)\n",
        "\n",
        "        fig = px.bar(\n",
        "            df_union,\n",
        "            x=col,\n",
        "            y='proportion',\n",
        "            color='dataset',\n",
        "            barmode='group',\n",
        "            title=f\"Category Proportions for '{col}'\"\n",
        "        )\n",
        "        logger.info(f\"  → Category bar chart ready for '{col}'.\")\n",
        "        return fig\n",
        "\n",
        "\n",
        "    def plot_text_top_tokens(self, col: str, top_n: int = 20):\n",
        "        logger.info(f\"Generating top‐token bar chart for '{col}' (top_n={top_n}).\")\n",
        "        texts_ref = self.ref_df[col].dropna().astype(str).tolist()\n",
        "        texts_cmp = self.cmp_df[col].dropna().astype(str).tolist()\n",
        "\n",
        "        tokens_ref = [tok for t in texts_ref for tok in t.split()]\n",
        "        tokens_cmp = [tok for t in texts_cmp for tok in t.split()]\n",
        "\n",
        "        ctr_ref = Counter(tokens_ref)\n",
        "        ctr_cmp = Counter(tokens_cmp)\n",
        "        combined = ctr_ref + ctr_cmp\n",
        "        most_common = [tok for tok, _ in combined.most_common(top_n)]\n",
        "\n",
        "        ref_freqs = [ctr_ref.get(tok, 0) for tok in most_common]\n",
        "        cmp_freqs = [ctr_cmp.get(tok, 0) for tok in most_common]\n",
        "\n",
        "        n = len(most_common)\n",
        "        df_plot = pd.DataFrame({\n",
        "            'token': most_common * 2,\n",
        "            'count': ref_freqs + cmp_freqs,\n",
        "            'dataset': ['Reference'] * n + ['Comparison'] * n\n",
        "        })\n",
        "\n",
        "        fig = px.bar(\n",
        "            df_plot,\n",
        "            x='token',\n",
        "            y='count',\n",
        "            color='dataset',\n",
        "            barmode='group',\n",
        "            title=f\"Top {n} Token Frequencies for '{col}'\"\n",
        "        )\n",
        "        logger.info(f\"  → Top‐token bar chart ready for '{col}'.\")\n",
        "        return fig\n",
        "\n",
        "\n",
        "    def plot_text_length_hist(self, col: str):\n",
        "        logger.info(f\"Generating document‐length histogram for '{col}'.\")\n",
        "        lens_ref = self.ref_df[col].dropna().astype(str).str.split().apply(len)\n",
        "        lens_cmp = self.cmp_df[col].dropna().astype(str).str.split().apply(len)\n",
        "\n",
        "        df_plot = pd.DataFrame({\n",
        "            'length': pd.concat([lens_ref, lens_cmp], ignore_index=True),\n",
        "            'dataset': ['Reference'] * len(lens_ref) + ['Comparison'] * len(lens_cmp)\n",
        "        })\n",
        "\n",
        "        fig = px.histogram(\n",
        "            df_plot,\n",
        "            x='length',\n",
        "            color='dataset',\n",
        "            histnorm='probability density',\n",
        "            barmode='overlay',\n",
        "            nbins=30,\n",
        "            title=f\"Document Length Distribution for '{col}'\"\n",
        "        )\n",
        "        logger.info(f\"  → Document‐length histogram ready for '{col}'.\")\n",
        "        return fig\n",
        "\n",
        "\n",
        "    def plot_text_embedding_scatter(self, col: str):\n",
        "        logger.info(f\"Generating (empty) embedding scatter for '{col}'.\")\n",
        "        # Placeholder (no implementation)\n",
        "        return go.Figure()\n",
        "\n",
        "\n",
        "    # ───── PCA‐Based Methods ────────────────────────────────────────────────────\n",
        "\n",
        "    def _get_numeric_matrix(self) -> (np.ndarray, np.ndarray, list):\n",
        "        logger.info(\"Building numeric matrices for PCA.\")\n",
        "        numeric_cols = [col for col in self.common_columns if self.column_types[col] == 'numeric']\n",
        "        logger.info(f\"  → Numeric columns: {numeric_cols}\")\n",
        "        if not numeric_cols:\n",
        "            return np.array([]), np.array([]), []\n",
        "\n",
        "        ref_num = self.ref_df[numeric_cols].dropna(axis=0, how='any').astype(float)\n",
        "        cmp_num = self.cmp_df[numeric_cols].dropna(axis=0, how='any').astype(float)\n",
        "        logger.info(f\"  → After dropping NaNs: ref_num shape={ref_num.shape}, cmp_num shape={cmp_num.shape}\")\n",
        "\n",
        "        scaler_ref = StandardScaler().fit(ref_num.values)\n",
        "        X_ref = scaler_ref.transform(ref_num.values)\n",
        "\n",
        "        scaler_cmp = StandardScaler().fit(cmp_num.values)\n",
        "        X_cmp = scaler_cmp.transform(cmp_num.values)\n",
        "\n",
        "        return X_ref, X_cmp, numeric_cols\n",
        "\n",
        "\n",
        "    def compute_pca_eigen_cosines(self, n_components: int = 5) -> dict:\n",
        "        logger.info(f\"Computing PCA eigenvector cosines (n_components={n_components}).\")\n",
        "        X_ref, X_cmp, numeric_cols = self._get_numeric_matrix()\n",
        "        if X_ref.size == 0 or X_cmp.size == 0:\n",
        "            logger.info(\"  → Not enough numeric data: returning empty PCA result.\")\n",
        "            return {\n",
        "                'cosine_similarities': [],\n",
        "                'explained_variance_ref': [],\n",
        "                'explained_variance_cmp': [],\n",
        "                'weighted_cosine': np.nan\n",
        "            }\n",
        "\n",
        "        pca_ref = PCA(n_components=n_components)\n",
        "        pca_cmp = PCA(n_components=n_components)\n",
        "        pca_ref.fit(X_ref)\n",
        "        pca_cmp.fit(X_cmp)\n",
        "\n",
        "        eigvecs_ref = pca_ref.components_\n",
        "        eigvecs_cmp = pca_cmp.components_\n",
        "\n",
        "        cosines = []\n",
        "        for i in range(len(eigvecs_ref)):\n",
        "            v1 = eigvecs_ref[i]\n",
        "            v2 = eigvecs_cmp[i]\n",
        "            dot = np.dot(v1, v2)\n",
        "            n1 = np.linalg.norm(v1)\n",
        "            n2 = np.linalg.norm(v2)\n",
        "            cos = dot / (n1 * n2) if (n1 > 0 and n2 > 0) else 0.0\n",
        "            cosines.append(cos)\n",
        "            logger.info(f\"  → PC{i+1} cosine = {cos:.6f}\")\n",
        "\n",
        "        weights = pca_ref.explained_variance_ratio_\n",
        "        weighted_cos = float(np.dot(cosines, weights[:len(cosines)]))\n",
        "        logger.info(f\"  → Weighted cosine = {weighted_cos:.6f}\")\n",
        "\n",
        "        return {\n",
        "            'cosine_similarities': cosines,\n",
        "            'explained_variance_ref': pca_ref.explained_variance_ratio_.tolist(),\n",
        "            'explained_variance_cmp': pca_cmp.explained_variance_ratio_.tolist(),\n",
        "            'weighted_cosine': weighted_cos\n",
        "        }\n",
        "\n",
        "\n",
        "    def get_pca_projection_dataframe(self, n_components: int = 2) -> pd.DataFrame:\n",
        "        logger.info(f\"Building PCA projection dataframe (n_components={n_components}).\")\n",
        "        X_ref, X_cmp, numeric_cols = self._get_numeric_matrix()\n",
        "        if X_ref.size == 0 or X_cmp.size == 0:\n",
        "            logger.info(\"  → Not enough numeric data: returning empty DataFrame.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        X_all = np.vstack([X_ref, X_cmp])\n",
        "        pca = PCA(n_components=n_components)\n",
        "        coords_all = pca.fit_transform(X_all)\n",
        "\n",
        "        n_ref = X_ref.shape[0]\n",
        "        df_coords = pd.DataFrame(coords_all, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
        "        df_coords['dataset'] = ['Reference'] * n_ref + ['Comparison'] * (coords_all.shape[0] - n_ref)\n",
        "        logger.info(\"  → PCA projection dataframe ready.\")\n",
        "        return df_coords\n",
        "\n",
        "\n",
        "    def plot_pca_scatter(self, n_components: int = 2):\n",
        "        logger.info(f\"Generating PCA scatter plot (components={n_components}).\")\n",
        "        df_coords = self.get_pca_projection_dataframe(n_components=n_components)\n",
        "        if df_coords.empty or 'PC1' not in df_coords or 'PC2' not in df_coords:\n",
        "            logger.info(\"  → Not enough PCA components: returning empty figure.\")\n",
        "            return go.Figure()\n",
        "\n",
        "        fig = px.scatter(\n",
        "            df_coords,\n",
        "            x=\"PC1\",\n",
        "            y=\"PC2\",\n",
        "            color=\"dataset\",\n",
        "            symbol=\"dataset\",\n",
        "            title=f\"PCA Projection (PC1 vs PC2)\"\n",
        "        )\n",
        "        fig.update_traces(marker=dict(size=5, opacity=0.75))\n",
        "        logger.info(\"  → PCA scatter plot ready.\")\n",
        "        return fig\n",
        "\n",
        "\n",
        "    def compute_correlation_similarity(self) -> float:\n",
        "        \"\"\"\n",
        "        Computes the average pairwise Pearson‐correlation similarity between\n",
        "        numeric columns in the reference vs. comparison data. If there are fewer\n",
        "        than 2 numeric columns, returns np.nan.\n",
        "        \"\"\"\n",
        "        logger.info(\"Computing global correlation similarity (numeric × numeric).\")\n",
        "        numeric_cols = [c for c in self.common_columns if self.column_types.get(c) == \"numeric\"]\n",
        "        logger.info(f\"  → Numeric columns: {numeric_cols}\")\n",
        "        if len(numeric_cols) < 2:\n",
        "            logger.info(\"  → Fewer than 2 numeric columns: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        df_real = self.ref_df[numeric_cols].dropna()\n",
        "        df_syn  = self.cmp_df[numeric_cols].dropna()\n",
        "        if df_real.shape[0] < 2 or df_syn.shape[0] < 2:\n",
        "            logger.info(\"  → Not enough rows after dropping NaNs: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        corr_real = df_real.corr(method=\"pearson\")\n",
        "        corr_syn  = df_syn.corr(method=\"pearson\")\n",
        "        sims = []\n",
        "        for (i, j) in combinations(numeric_cols, 2):\n",
        "            r_r = corr_real.at[i, j]\n",
        "            r_s = corr_syn.at[i, j]\n",
        "            if pd.isna(r_r) or pd.isna(r_s):\n",
        "                continue\n",
        "            sim_ij = 1.0 - abs(r_r - r_s)\n",
        "            sims.append(sim_ij)\n",
        "            logger.info(f\"  → Pair ({i},{j}): corr_real={r_r:.4f}, corr_syn={r_s:.4f}, sim={sim_ij:.6f}\")\n",
        "\n",
        "        if not sims:\n",
        "            logger.info(\"  → No valid correlation pairs: returning NaN.\")\n",
        "            return np.nan\n",
        "        mean_sim = float(np.mean(sims))\n",
        "        logger.info(f\"  → Mean correlation similarity = {mean_sim:.6f}\")\n",
        "        return mean_sim\n",
        "\n",
        "\n",
        "    def compute_contingency_similarity(self) -> float:\n",
        "        \"\"\"\n",
        "        Computes the average Total Variation Distance (TVD) similarity over all\n",
        "        pairs of categorical columns. If fewer than 2 categorical columns, returns np.nan.\n",
        "\n",
        "        For each pair of categorical columns (A,B):\n",
        "          • Build the joint frequency table in real and synthetic (normalized).\n",
        "          • TVD(A,B) = 0.5 * sum |p_real( a,b ) - p_syn( a,b )| over all (a,b).\n",
        "          • sim(A,B) = 1 - TVD(A,B).\n",
        "        Finally, return the mean(sim(A,B)) for all A < B.\n",
        "        \"\"\"\n",
        "        logger.info(\"Computing global contingency similarity (categorical × categorical).\")\n",
        "        categorical_cols = [c for c in self.common_columns if self.column_types.get(c) == \"categorical\"]\n",
        "        logger.info(f\"  → Categorical columns: {categorical_cols}\")\n",
        "        if len(categorical_cols) < 2:\n",
        "            logger.info(\"  → Fewer than 2 categorical columns: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        sims = []\n",
        "        for (c1, c2) in combinations(categorical_cols, 2):\n",
        "            logger.info(f\"  → Processing pair ({c1},{c2}).\")\n",
        "            real_pair = self.ref_df[[c1, c2]].dropna().astype(str)\n",
        "            syn_pair  = self.cmp_df[[c1, c2]].dropna().astype(str)\n",
        "\n",
        "            if real_pair.shape[0] < 1 or syn_pair.shape[0] < 1:\n",
        "                logger.info(f\"    → One side empty after dropna: skipping pair.\")\n",
        "                continue\n",
        "\n",
        "            real_counts = real_pair.groupby([c1, c2]).size().rename(\"count\").reset_index()\n",
        "            syn_counts  = syn_pair.groupby([c1, c2]).size().rename(\"count\").reset_index()\n",
        "\n",
        "            real_pivot = real_counts.pivot(index=c1, columns=c2, values=\"count\").fillna(0)\n",
        "            syn_pivot  = syn_counts.pivot(index=c1, columns=c2, values=\"count\").fillna(0)\n",
        "\n",
        "            all_index = real_pivot.index.union(syn_pivot.index)\n",
        "            all_columns = real_pivot.columns.union(syn_pivot.columns)\n",
        "            real_norm = real_pivot.reindex(index=all_index, columns=all_columns, fill_value=0)\n",
        "            syn_norm  = syn_pivot.reindex(index=all_index, columns=all_columns, fill_value=0)\n",
        "\n",
        "            real_prob = real_norm.values / real_norm.values.sum()\n",
        "            syn_prob  = syn_norm.values / syn_norm.values.sum()\n",
        "\n",
        "            tvd = 0.5 * np.abs(real_prob - syn_prob).sum()\n",
        "            sim = 1.0 - tvd\n",
        "            sims.append(sim)\n",
        "            logger.info(f\"    → TVD for pair = {tvd:.6f}, similarity = {sim:.6f}\")\n",
        "\n",
        "        if not sims:\n",
        "            logger.info(\"  → No valid categorical pairs: returning NaN.\")\n",
        "            return np.nan\n",
        "        mean_sim = float(np.mean(sims))\n",
        "        logger.info(f\"  → Mean contingency similarity = {mean_sim:.6f}\")\n",
        "        return mean_sim\n",
        "\n",
        "\n",
        "    def compute_distinguishability_auc(\n",
        "        self,\n",
        "        test_size: float = 0.3,\n",
        "        random_state: int = 42,\n",
        "        max_rows_per_side: int = 1000\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Trains a LogisticRegression on a balanced subsample\n",
        "        of up to max_rows_per_side real vs. synthetic rows.\n",
        "        Returns ROC AUC. If not enough variance, returns NaN.\n",
        "        \"\"\"\n",
        "        logger.info(f\"Computing distinguishability AUC (max_rows_per_side={max_rows_per_side}).\")\n",
        "        df_real = self.ref_df.copy()\n",
        "        df_real[\"_is_synthetic\"] = 0\n",
        "        df_syn  = self.cmp_df.copy()\n",
        "        df_syn[\"_is_synthetic\"] = 1\n",
        "\n",
        "        if len(df_real) > max_rows_per_side:\n",
        "            df_real = df_real.sample(n=max_rows_per_side, random_state=random_state)\n",
        "            logger.info(f\"  → Subsampled real to {max_rows_per_side} rows.\")\n",
        "        if len(df_syn) > max_rows_per_side:\n",
        "            df_syn = df_syn.sample(n=max_rows_per_side, random_state=random_state)\n",
        "            logger.info(f\"  → Subsampled synthetic to {max_rows_per_side} rows.\")\n",
        "\n",
        "        df_all = pd.concat([df_real, df_syn], ignore_index=True)\n",
        "        logger.info(f\"  → Concatenated dataset shape = {df_all.shape}\")\n",
        "\n",
        "        numeric_cols = [c for c in self.common_columns if self.column_types.get(c) == \"numeric\"]\n",
        "        categorical_cols = [c for c in self.common_columns if self.column_types.get(c) == \"categorical\"]\n",
        "        logger.info(f\"  → Numeric columns: {numeric_cols}\")\n",
        "        logger.info(f\"  → Categorical columns: {categorical_cols}\")\n",
        "\n",
        "        if numeric_cols:\n",
        "            num_mat = df_all[numeric_cols].astype(float).fillna(0.0)\n",
        "            scaler = StandardScaler()\n",
        "            X_num = scaler.fit_transform(num_mat.values)\n",
        "            logger.info(f\"  → Numeric matrix shape = {X_num.shape}\")\n",
        "        else:\n",
        "            X_num = np.empty((len(df_all), 0))\n",
        "            logger.info(\"  → No numeric columns: X_num is empty array.\")\n",
        "\n",
        "        if categorical_cols:\n",
        "            df_cat = df_all[categorical_cols].astype(str).fillna(\"NA\").copy()\n",
        "            for col in categorical_cols:\n",
        "                top_cats = df_cat[col].value_counts().nlargest(50).index\n",
        "                df_cat[col] = df_cat[col].where(df_cat[col].isin(top_cats), other=\"OTHER\")\n",
        "                logger.info(f\"  → For '{col}', collapsed rare categories to 'OTHER'.\")\n",
        "            encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "            X_cat = encoder.fit_transform(df_cat.values)\n",
        "            logger.info(f\"  → One-hot encoded shape = {X_cat.shape}\")\n",
        "        else:\n",
        "            X_cat = np.empty((len(df_all), 0))\n",
        "            logger.info(\"  → No categorical columns: X_cat is empty array.\")\n",
        "\n",
        "        X = np.hstack([X_num, X_cat])\n",
        "        y = df_all[\"_is_synthetic\"].values\n",
        "        logger.info(f\"  → Combined feature matrix shape = {X.shape}, labels length = {len(y)}\")\n",
        "\n",
        "        if len(np.unique(y)) < 2:\n",
        "            logger.info(\"  → Only one class present: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        try:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=test_size, stratify=y, random_state=random_state\n",
        "            )\n",
        "            logger.info(f\"  → Train/test split: X_train={X_train.shape}, X_test={X_test.shape}\")\n",
        "        except ValueError as e:\n",
        "            logger.info(f\"  → Train/test split error: {e}. Returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        clf = LogisticRegression(\n",
        "            solver=\"liblinear\", max_iter=100, random_state=random_state\n",
        "        )\n",
        "        clf.fit(X_train, y_train)\n",
        "        logger.info(\"  → LogisticRegression fitted on training data.\")\n",
        "\n",
        "        y_prob = clf.predict_proba(X_test)[:, 1]\n",
        "        try:\n",
        "            auc = roc_auc_score(y_test, y_prob)\n",
        "            logger.info(f\"  → Computed ROC AUC = {auc:.6f}\")\n",
        "        except ValueError as e:\n",
        "            logger.info(f\"  → ROC AUC computation error: {e}. Returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        return float(auc)\n",
        "\n",
        "\n",
        "    # ─── Text‐Bigram & OOV Helpers ────────────────────────────────────────────\n",
        "\n",
        "    def _compute_text_bigram_jaccard(self, col: str) -> float:\n",
        "        logger.info(f\"Computing bigram‐level Jaccard for '{col}'.\")\n",
        "        texts_ref = self.ref_df[col].dropna().astype(str).tolist()\n",
        "        texts_cmp = self.cmp_df[col].dropna().astype(str).tolist()\n",
        "        if not texts_ref or not texts_cmp:\n",
        "            logger.info(\"  → One side empty: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        def all_bigrams(texts):\n",
        "            bigrams = set()\n",
        "            for t in texts:\n",
        "                tokens = t.split()\n",
        "                for i in range(len(tokens) - 1):\n",
        "                    bigrams.add((tokens[i], tokens[i + 1]))\n",
        "            return bigrams\n",
        "\n",
        "        big_ref = all_bigrams(texts_ref)\n",
        "        big_cmp = all_bigrams(texts_cmp)\n",
        "        if not big_ref and not big_cmp:\n",
        "            logger.info(\"  → Both bigram sets empty: returning NaN.\")\n",
        "            return np.nan\n",
        "        if not big_ref or not big_cmp:\n",
        "            logger.info(\"  → One bigram set empty: returning 0.0.\")\n",
        "            return 0.0\n",
        "        intersect = big_ref.intersection(big_cmp)\n",
        "        union = big_ref.union(big_cmp)\n",
        "        jaccard = len(intersect) / len(union)\n",
        "        logger.info(f\"  → Bigram Jaccard = {jaccard:.6f}\")\n",
        "        return jaccard\n",
        "\n",
        "\n",
        "    def _compute_text_bigram_js(self, col: str, top_k: int = 5000) -> float:\n",
        "        logger.info(f\"Computing bigram‐level JS divergence for '{col}'.\")\n",
        "        texts_ref = self.ref_df[col].dropna().astype(str).tolist()\n",
        "        texts_cmp = self.cmp_df[col].dropna().astype(str).tolist()\n",
        "        if not texts_ref or not texts_cmp:\n",
        "            logger.info(\"  → One side empty: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        def bigram_counts(texts):\n",
        "            ctr = Counter()\n",
        "            for t in texts:\n",
        "                tokens = t.split()\n",
        "                for i in range(len(tokens) - 1):\n",
        "                    ctr[(tokens[i], tokens[i + 1])] += 1\n",
        "            return ctr\n",
        "\n",
        "        ctr_ref = bigram_counts(texts_ref)\n",
        "        ctr_cmp = bigram_counts(texts_cmp)\n",
        "        combined = ctr_ref + ctr_cmp\n",
        "        most_common = [bg for bg, _ in combined.most_common(top_k)]\n",
        "        logger.info(f\"  → Top {len(most_common)} bigrams selected.\")\n",
        "\n",
        "        p = np.array([ctr_ref.get(bg, 0) for bg in most_common], dtype=float)\n",
        "        q = np.array([ctr_cmp.get(bg, 0) for bg in most_common], dtype=float)\n",
        "        if p.sum() == 0 or q.sum() == 0:\n",
        "            logger.info(\"  → Zero counts on one side: returning NaN.\")\n",
        "            return np.nan\n",
        "        p = p / p.sum()\n",
        "        q = q / q.sum()\n",
        "        js = Report._js_divergence(p, q)\n",
        "        return js\n",
        "\n",
        "\n",
        "    def _compute_text_oov_rate(self, col: str) -> float:\n",
        "        logger.info(f\"Computing OOV rate for '{col}'.\")\n",
        "        texts_ref = self.ref_df[col].dropna().astype(str).tolist()\n",
        "        texts_cmp = self.cmp_df[col].dropna().astype(str).tolist()\n",
        "        if not texts_ref or not texts_cmp:\n",
        "            logger.info(\"  → One side empty: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        tokens_ref = set(tok for t in texts_ref for tok in t.split())\n",
        "        tokens_cmp = set(tok for t in texts_cmp for tok in t.split())\n",
        "        if not tokens_ref:\n",
        "            logger.info(\"  → No tokens in real: returning NaN.\")\n",
        "            return np.nan\n",
        "        oov_real = len(tokens_ref - tokens_cmp) / len(tokens_ref)\n",
        "        logger.info(f\"  → OOV rate (real→syn) = {oov_real:.6f}\")\n",
        "        return oov_real\n",
        "\n",
        "\n",
        "    def _compute_text_length_stats(self, col: str) -> dict:\n",
        "        logger.info(f\"Computing text length stats for '{col}'.\")\n",
        "        lens_ref = self.ref_df[col].dropna().astype(str).str.split().apply(len)\n",
        "        lens_cmp = self.cmp_df[col].dropna().astype(str).str.split().apply(len)\n",
        "        if lens_ref.empty or lens_cmp.empty:\n",
        "            logger.info(\"  → One side empty: returning NaN stats.\")\n",
        "            return {'len_real': np.nan, 'len_syn': np.nan, 'len_diff': np.nan}\n",
        "        avg_r = lens_ref.mean()\n",
        "        avg_s = lens_cmp.mean()\n",
        "        diff = avg_s - avg_r\n",
        "        logger.info(f\"  → Avg length real={avg_r:.2f}, syn={avg_s:.2f}, diff={diff:.2f}\")\n",
        "        return {'len_real': avg_r, 'len_syn': avg_s, 'len_diff': diff}\n",
        "\n",
        "\n",
        "    def _compute_text_topic_cosine(\n",
        "        self,\n",
        "        col: str,\n",
        "        n_topics: int = 5,\n",
        "        max_features: int = 1000,\n",
        "        max_docs_per_side: int = 200\n",
        "    ) -> float:\n",
        "        logger.info(f\"Computing topic cosine for '{col}'.\")\n",
        "        texts_ref = self.ref_df[col].dropna().astype(str).tolist()\n",
        "        texts_cmp = self.cmp_df[col].dropna().astype(str).tolist()\n",
        "        if not texts_ref or not texts_cmp:\n",
        "            logger.info(\"  → One side empty: returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        if len(texts_ref) > max_docs_per_side:\n",
        "            rng = np.random.default_rng(seed=0)\n",
        "            texts_ref = list(rng.choice(texts_ref, max_docs_per_side, replace=False))\n",
        "            logger.info(f\"  → Subsampled {len(texts_ref)} real docs.\")\n",
        "        if len(texts_cmp) > max_docs_per_side:\n",
        "            rng = np.random.default_rng(seed=1)\n",
        "            texts_cmp = list(rng.choice(texts_cmp, max_docs_per_side, replace=False))\n",
        "            logger.info(f\"  → Subsampled {len(texts_cmp)} synthetic docs.\")\n",
        "\n",
        "        corpus = texts_ref + texts_cmp\n",
        "        logger.info(f\"  → TF–IDF on {len(corpus)} docs (max_features={max_features}).\")\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            max_features=max_features,\n",
        "            stop_words='english'\n",
        "        )\n",
        "        try:\n",
        "            tfidf_all = vectorizer.fit_transform(corpus)\n",
        "        except Exception as e:\n",
        "            logger.info(f\"  → TF–IDF fit error: {e}. Returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        nmf = NMF(\n",
        "            n_components=n_topics,\n",
        "            random_state=0,\n",
        "            init='nndsvda',\n",
        "            max_iter=100\n",
        "        )\n",
        "        logger.info(f\"  → Fitting NMF (n_topics={n_topics}) on TF–IDF matrix.\")\n",
        "        try:\n",
        "            W_all = nmf.fit_transform(tfidf_all)\n",
        "        except Exception as e:\n",
        "            logger.info(f\"  → NMF fit error: {e}. Returning NaN.\")\n",
        "            return np.nan\n",
        "\n",
        "        n_ref = len(texts_ref)\n",
        "        W_ref = W_all[:n_ref, :]\n",
        "        W_cmp = W_all[n_ref:, :]\n",
        "\n",
        "        mean_ref = W_ref.mean(axis=0)\n",
        "        mean_cmp = W_cmp.mean(axis=0)\n",
        "        denom = np.linalg.norm(mean_ref) * np.linalg.norm(mean_cmp)\n",
        "        cosine = float(np.dot(mean_ref, mean_cmp) / denom) if denom > 0 else np.nan\n",
        "        logger.info(f\"  → Topic cosine = {cosine:.6f}\")\n",
        "        return cosine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tab (Abstract)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tab(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base for a UI “tab.” Each subclass must implement build_ui() and register_callbacks().\n",
        "    \"\"\"\n",
        "    def __init__(self, report_state, common_cols_state):\n",
        "        self.report_state = report_state\n",
        "        self.common_cols_state = common_cols_state\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_ui(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def register_callbacks(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GeneralStatsTab(Tab):\n",
        "    \"\"\"\n",
        "    “General Stats” tab, reorganised into accordions for:\n",
        "      • Missingness\n",
        "      • Numeric Distributions\n",
        "      • Categorical Frequencies\n",
        "      • Text Metadata\n",
        "    \"\"\"\n",
        "    def __init__(self, report_state, common_cols_state):\n",
        "        super().__init__(report_state, common_cols_state)\n",
        "\n",
        "    def build_ui(self):\n",
        "        with gr.TabItem(\"General Stats\"):\n",
        "            # ---- Data‐Load Controls ----\n",
        "            with gr.Row():\n",
        "                self.ref_path_input = gr.Textbox(\n",
        "                    label=\"Reference Dataset Path\", placeholder=\"e.g. data/reference.csv\"\n",
        "                )\n",
        "                self.cmp_path_input = gr.Textbox(\n",
        "                    label=\"Comparison Dataset Path\", placeholder=\"e.g. data/comparison.csv\"\n",
        "                )\n",
        "            with gr.Row():\n",
        "                self.ref_schema_input = gr.Textbox(\n",
        "                    label=\"Reference Schema Path (JSON/YAML)\", placeholder=\"e.g. data/ref_schema.json\"\n",
        "                )\n",
        "                self.cmp_schema_input = gr.Textbox(\n",
        "                    label=\"Comparison Schema Path (JSON/YAML)\", placeholder=\"e.g. data/cmp_schema.json\"\n",
        "                )\n",
        "\n",
        "            self.load_button = gr.Button(\"Load Data\")\n",
        "            self.load_status = gr.Markdown(\"Awaiting data load...\")\n",
        "\n",
        "            # ---- Side‐by‐Side Dataframes of Common Columns ----\n",
        "            with gr.Row():\n",
        "                self.ref_table = gr.Dataframe(\n",
        "                    value=None,\n",
        "                    label=\"Reference Dataset (Common Columns Only)\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                self.cmp_table = gr.Dataframe(\n",
        "                    value=None,\n",
        "                    label=\"Comparison Dataset (Common Columns Only)\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            # ---- Common‐Columns Dropdown ----\n",
        "            self.column_dropdown = gr.Dropdown(\n",
        "                label=\"Select Common Column for Univariate Comparison\",\n",
        "                choices=[],\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            # ---- Accordions & Placeholders ----\n",
        "\n",
        "            # 1) Missingness Accordion\n",
        "            self.missing_accordion = gr.Accordion(\"Missingness\", visible=False, open=True)\n",
        "            with self.missing_accordion:\n",
        "                self.missing_md = gr.Markdown(\"\", visible=False)\n",
        "\n",
        "            # 2) Numeric Distributions Accordion\n",
        "            self.numeric_accordion = gr.Accordion(\"Numeric Distributions\", visible=False, open=False)\n",
        "            with self.numeric_accordion:\n",
        "                self.numeric_md        = gr.Markdown(\"\", visible=False)\n",
        "                self.numeric_cdf_plot  = gr.Plot(visible=False)\n",
        "                self.numeric_hist_plot = gr.Plot(visible=False)\n",
        "                self.numeric_kde_plot  = gr.Plot(visible=False)\n",
        "\n",
        "            # 3) Categorical Frequencies Accordion\n",
        "            self.categorical_accordion = gr.Accordion(\"Categorical Frequencies\", visible=False, open=False)\n",
        "            with self.categorical_accordion:\n",
        "                self.categorical_md       = gr.Markdown(\"\", visible=False)\n",
        "                self.categorical_bar_plot = gr.Plot(visible=False)\n",
        "\n",
        "            # 4) Text Metadata Accordion\n",
        "            self.text_accordion = gr.Accordion(\"Text Metadata\", visible=False, open=False)\n",
        "            with self.text_accordion:\n",
        "                self.text_md            = gr.Markdown(\"\", visible=False)\n",
        "                self.text_token_plot    = gr.Plot(visible=False)\n",
        "                self.text_len_plot      = gr.Plot(visible=False)\n",
        "                self.text_samples_real  = gr.Textbox(label=\"Sample Real Text\", visible=False, interactive=False)\n",
        "                self.text_samples_synth = gr.Textbox(label=\"Sample Synthetic Text\", visible=False, interactive=False)\n",
        "\n",
        "    def register_callbacks(self):\n",
        "        # 1. Load Data callback\n",
        "        def load_data(ref_path, cmp_path, ref_schema_path, cmp_schema_path):\n",
        "            try:\n",
        "                ref_df, cmp_df, common_columns, ref_schema, cmp_schema = (\n",
        "                    DataManager()\n",
        "                    .get_ref_and_cmp_data(\n",
        "                        ref_path, cmp_path, ref_schema_path, cmp_schema_path\n",
        "                    )\n",
        "                )\n",
        "                report = Report(ref_df, cmp_df, common_columns, ref_schema, cmp_schema)\n",
        "\n",
        "                ref_sub = ref_df.loc[:, common_columns]\n",
        "                cmp_sub = cmp_df.loc[:, common_columns]\n",
        "\n",
        "                return (\n",
        "                    report,\n",
        "                    common_columns,\n",
        "                    ref_sub,\n",
        "                    cmp_sub,\n",
        "                    f\"✅ Data loaded successfully. Found {len(common_columns)} common columns.\"\n",
        "                )\n",
        "            except Exception as e:\n",
        "                err = f\"❌ Error loading data: {e}\"\n",
        "                return (None, None, None, None, err)\n",
        "\n",
        "        self.load_button.click(\n",
        "            load_data,\n",
        "            inputs=[\n",
        "                self.ref_path_input,\n",
        "                self.cmp_path_input,\n",
        "                self.ref_schema_input,\n",
        "                self.cmp_schema_input,\n",
        "            ],\n",
        "            outputs=[\n",
        "                self.report_state,\n",
        "                self.common_cols_state,\n",
        "                self.ref_table,\n",
        "                self.cmp_table,\n",
        "                self.load_status,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # 2. Enable Dataframes & Dropdown after data loads\n",
        "        def enable_components(report, common_columns):\n",
        "            if report is not None and common_columns:\n",
        "                return (\n",
        "                    gr.update(interactive=True),\n",
        "                    gr.update(interactive=True),\n",
        "                    gr.update(choices=common_columns, interactive=True),\n",
        "                )\n",
        "            else:\n",
        "                return (\n",
        "                    gr.update(interactive=False),\n",
        "                    gr.update(interactive=False),\n",
        "                    gr.update(choices=[], interactive=False),\n",
        "                )\n",
        "\n",
        "        self.report_state.change(\n",
        "            enable_components,\n",
        "            inputs=[self.report_state, self.common_cols_state],\n",
        "            outputs=[self.ref_table, self.cmp_table, self.column_dropdown]\n",
        "        )\n",
        "\n",
        "        # 3. Show per‐column stats & plots when dropdown changes\n",
        "        def show_column_stats(report, col_name):\n",
        "            # If no report or no column chosen: hide everything\n",
        "            if report is None or not col_name:\n",
        "                return (\n",
        "                    # Missingness Accordion + content\n",
        "                    gr.update(visible=False),\n",
        "                    \"\", gr.update(visible=False),\n",
        "\n",
        "                    # Numeric Accordion + content + plots\n",
        "                    gr.update(visible=False),\n",
        "                    \"\", gr.update(visible=False),\n",
        "                    None, gr.update(visible=False),\n",
        "                    None, gr.update(visible=False),\n",
        "                    None, gr.update(visible=False),\n",
        "\n",
        "                    # Categorical Accordion + content + plot\n",
        "                    gr.update(visible=False),\n",
        "                    \"\", gr.update(visible=False),\n",
        "                    None, gr.update(visible=False),\n",
        "\n",
        "                    # Text Accordion + content + plots + samples\n",
        "                    gr.update(visible=False),\n",
        "                    \"\", gr.update(visible=False),\n",
        "                    None, gr.update(visible=False),\n",
        "                    None, gr.update(visible=False),\n",
        "                    \"\", gr.update(visible=False),\n",
        "                    \"\", gr.update(visible=False),\n",
        "                )\n",
        "\n",
        "            # Compute univariate metrics\n",
        "            ctype       = report.column_types.get(col_name)\n",
        "            col_metrics = report.compute_column(col_name)\n",
        "\n",
        "            #######  (A) MISSINGNESS #######\n",
        "            mr_real = report.ref_df[col_name].isna().mean()\n",
        "            mr_synth = report.cmp_df[col_name].isna().mean()\n",
        "            mr_diff = abs(mr_real - mr_synth)\n",
        "            missing_md = \"\"\n",
        "            missing_md += \"### Missingness (Real vs Synthetic)\\n\\n\"\n",
        "            missing_md += (\n",
        "                f\"- Real % missing = {mr_real*100:.1f}%  |  \"\n",
        "                f\"Synth % missing = {mr_synth*100:.1f}%  |  \"\n",
        "                f\"Δ = {mr_diff*100:+.1f} pts\\n\"\n",
        "            )\n",
        "\n",
        "            #######  (B) NUMERIC #######\n",
        "            numeric_md   = \"\"\n",
        "            cdf_fig      = None\n",
        "            hist_fig     = None\n",
        "            kde_fig      = None\n",
        "            show_numeric = (ctype == \"numeric\")\n",
        "\n",
        "            if show_numeric:\n",
        "                # 1) KS similarity\n",
        "                ks_sim = col_metrics.get(\"ks_similarity\", np.nan)\n",
        "                # 2) Wasserstein (raw + normalised)\n",
        "                w_raw  = col_metrics.get(\"wasserstein_raw\",   np.nan)\n",
        "                w_norm = col_metrics.get(\"wasserstein_norm\",  np.nan)\n",
        "                # 3) Mean / Median / Std differences\n",
        "                mean_r   = col_metrics.get(\"mean_real\",    np.nan)\n",
        "                mean_s   = col_metrics.get(\"mean_syn\",     np.nan)\n",
        "                mean_d   = col_metrics.get(\"mean_diff\",    np.nan)\n",
        "                mean_pct = col_metrics.get(\"mean_pct_of_range\", np.nan)\n",
        "\n",
        "                med_r    = col_metrics.get(\"median_real\",  np.nan)\n",
        "                med_s    = col_metrics.get(\"median_syn\",   np.nan)\n",
        "                med_d    = col_metrics.get(\"median_diff\",  np.nan)\n",
        "                med_pct  = col_metrics.get(\"median_pct_of_range\", np.nan)\n",
        "\n",
        "                std_r    = col_metrics.get(\"std_real\",     np.nan)\n",
        "                std_s    = col_metrics.get(\"std_syn\",      np.nan)\n",
        "                std_d    = col_metrics.get(\"std_diff\",     np.nan)\n",
        "                std_pct  = col_metrics.get(\"std_pct_of_std\", np.nan)\n",
        "\n",
        "                # 4) Range coverage\n",
        "                range_cov = col_metrics.get(\"range_coverage\", np.nan)\n",
        "\n",
        "                numeric_md += \"### Numeric Distributions (Real vs Synthetic)\\n\\n\"\n",
        "                if not np.isnan(ks_sim):\n",
        "                    numeric_md += f\"- KS Similarity (1 – D): **{ks_sim:.4f}**\\n\"\n",
        "                if not np.isnan(w_raw):\n",
        "                    if not np.isnan(w_norm):\n",
        "                        numeric_md += (\n",
        "                            f\"- Wasserstein Distance: **{w_raw:.4f} units**  \"\n",
        "                            f\"(normalised = {w_norm:.4f})\\n\"\n",
        "                        )\n",
        "                    else:\n",
        "                        numeric_md += f\"- Wasserstein Distance: **{w_raw:.4f} units**  (normalised = N/A)\\n\"\n",
        "\n",
        "                if not np.isnan(mean_r) and not np.isnan(mean_s):\n",
        "                    pct_txt = f\" ({mean_pct*100:+.2f}% of real‐range)\" if not np.isnan(mean_pct) else \"\"\n",
        "                    numeric_md += (\n",
        "                        f\"- Mean → Real: **{mean_r:.4f}**, Syn: **{mean_s:.4f}**, \"\n",
        "                        f\"Δ = **{mean_d:+.4f}**{pct_txt}\\n\"\n",
        "                    )\n",
        "                if not np.isnan(med_r) and not np.isnan(med_s):\n",
        "                    pct_txt = f\" ({med_pct*100:+.2f}% of real‐range)\" if not np.isnan(med_pct) else \"\"\n",
        "                    numeric_md += (\n",
        "                        f\"- Median → Real: **{med_r:.4f}**, Syn: **{med_s:.4f}**, \"\n",
        "                        f\"Δ = **{med_d:+.4f}**{pct_txt}\\n\"\n",
        "                    )\n",
        "                if not np.isnan(std_r) and not np.isnan(std_s):\n",
        "                    pct_txt = f\" ({std_pct*100:+.1f}% of real‐std)\" if not np.isnan(std_pct) else \"\"\n",
        "                    numeric_md += (\n",
        "                        f\"- Std Dev → Real: **{std_r:.4f}**, Syn: **{std_s:.4f}**, \"\n",
        "                        f\"Δ = **{std_d:+.4f}**{pct_txt}\\n\"\n",
        "                    )\n",
        "\n",
        "                if not np.isnan(range_cov):\n",
        "                    numeric_md += f\"- Range Coverage: **{range_cov:.4f}** (1.0 = perfect)\\n\"\n",
        "\n",
        "                # Generate plots\n",
        "                cdf_fig  = report.plot_column_cdf(col_name)\n",
        "                hist_fig = report.plot_column_histogram(col_name)\n",
        "                kde_fig  = report.plot_column_kde(col_name)\n",
        "\n",
        "            #######  (C) CATEGORICAL #######\n",
        "            cat_md   = \"\"\n",
        "            cat_fig  = None\n",
        "            show_cat = (ctype == \"categorical\")\n",
        "\n",
        "            if show_cat:\n",
        "                tvd_sim  = col_metrics.get(\"tvd_similarity\",    np.nan)\n",
        "                coverage = col_metrics.get(\"category_coverage\", np.nan)\n",
        "\n",
        "                cat_md += \"### Categorical Frequencies (Real vs Synthetic)\\n\\n\"\n",
        "                if not np.isnan(tvd_sim):\n",
        "                    cat_md += f\"- TVD Similarity: **{tvd_sim:.4f}**\\n\"\n",
        "                if not np.isnan(coverage):\n",
        "                    cat_md += f\"- Category Coverage: **{coverage:.4f}**\\n\"\n",
        "\n",
        "                cat_fig = report.plot_category_bar(col_name)\n",
        "\n",
        "            #######  (D) TEXT #######\n",
        "            text_md    = \"\"\n",
        "            tok_fig    = None\n",
        "            len_fig    = None\n",
        "            samples_r  = \"\"\n",
        "            samples_s  = \"\"\n",
        "            show_text  = (ctype == \"text\")\n",
        "\n",
        "            if show_text:\n",
        "                tok_js     = col_metrics.get(\"text_tok_js\",        np.nan)\n",
        "                bigram_j   = col_metrics.get(\"text_bigram_jaccard\", np.nan)\n",
        "                bigram_js  = col_metrics.get(\"text_bigram_js\",      np.nan)\n",
        "                tfidf_cos  = col_metrics.get(\"text_tfidf_cosine\",   np.nan)\n",
        "                vocab_j    = col_metrics.get(\"text_vocab_jaccard\",  np.nan)\n",
        "                oov_rate   = col_metrics.get(\"text_oov_rate\",       np.nan)\n",
        "                topic_cos  = col_metrics.get(\"text_topic_cosine\",   np.nan)\n",
        "                len_r      = col_metrics.get(\"len_real\",            np.nan)\n",
        "                len_s      = col_metrics.get(\"len_syn\",             np.nan)\n",
        "                len_d      = col_metrics.get(\"len_diff\",            np.nan)\n",
        "                emb_cos    = col_metrics.get(\"text_emb_cosine\",     np.nan)\n",
        "                emb_mmd    = col_metrics.get(\"text_emb_mmd\",        np.nan)\n",
        "\n",
        "                text_md += \"### Text Metadata (Real vs Synthetic)\\n\\n\"\n",
        "                if not np.isnan(tok_js):\n",
        "                    text_md += f\"- Token‐JS Divergence (unigrams): **{tok_js:.4f}**\\n\"\n",
        "                if not np.isnan(bigram_j):\n",
        "                    text_md += f\"- Bigram Jaccard: **{bigram_j:.4f}**\\n\"\n",
        "                if not np.isnan(bigram_js):\n",
        "                    text_md += f\"- Bigram‐JS Divergence: **{bigram_js:.4f}**\\n\"\n",
        "                if not np.isnan(tfidf_cos):\n",
        "                    text_md += f\"- TF–IDF Cosine Similarity: **{tfidf_cos:.4f}**\\n\"\n",
        "                if not np.isnan(vocab_j):\n",
        "                    text_md += f\"- Vocabulary Jaccard (unigrams): **{vocab_j:.4f}**\\n\"\n",
        "                if not np.isnan(oov_rate):\n",
        "                    text_md += f\"- OOV Rate (real→synth): **{oov_rate*100:.1f}%**\\n\"\n",
        "                if not np.isnan(topic_cos):\n",
        "                    text_md += f\"- Topic‐Distribution Cosine: **{topic_cos:.4f}**\\n\"\n",
        "                if not np.isnan(len_r) and not np.isnan(len_s):\n",
        "                    text_md += (\n",
        "                        f\"- Average Length → Real: **{len_r:.1f}** tokens, \"\n",
        "                        f\"Synthetic: **{len_s:.1f}**, Δ = **{len_d:+.1f} tokens**\\n\"\n",
        "                    )\n",
        "                if not np.isnan(emb_cos):\n",
        "                    text_md += f\"- Embedding Cosine: **{emb_cos:.4f}**\\n\"\n",
        "                if not np.isnan(emb_mmd):\n",
        "                    text_md += f\"- Embedding MMD: **{emb_mmd:.4f}**\\n\"\n",
        "\n",
        "                # Plots for text\n",
        "                tok_fig = report.plot_text_top_tokens(col_name)\n",
        "                len_fig = report.plot_text_length_hist(col_name)\n",
        "\n",
        "                # Sample up to 3 real vs. synthetic texts\n",
        "                real_texts = report.ref_df[col_name].dropna().astype(str)\n",
        "                synth_texts = report.cmp_df[col_name].dropna().astype(str)\n",
        "                if len(real_texts) > 0 and len(synth_texts) > 0:\n",
        "                    samples_r = \"\\n\\n\".join(real_texts.sample(min(3, len(real_texts))).tolist())\n",
        "                    samples_s = \"\\n\\n\".join(synth_texts.sample(min(3, len(synth_texts))).tolist())\n",
        "\n",
        "            # Return all 22 outputs in the exact order declared in build_ui()\n",
        "            return (\n",
        "                # 1) Missingness Accordion visibility\n",
        "                gr.update(visible=True),\n",
        "                # 2) Missingness Markdown content\n",
        "                missing_md,\n",
        "                # 3) Missingness Markdown visibility\n",
        "                gr.update(visible=True),\n",
        "\n",
        "                # 4) Numeric Accordion visibility\n",
        "                gr.update(visible=show_numeric),\n",
        "                # 5) Numeric Markdown content\n",
        "                numeric_md,\n",
        "                # 6) Numeric Markdown visibility\n",
        "                gr.update(visible=show_numeric),\n",
        "                # 7) CDF plot (numeric)\n",
        "                cdf_fig,\n",
        "                gr.update(visible=show_numeric),\n",
        "                # 8) Histogram plot (numeric)\n",
        "                hist_fig,\n",
        "                gr.update(visible=show_numeric),\n",
        "                # 9) KDE plot (numeric)\n",
        "                kde_fig,\n",
        "                gr.update(visible=show_numeric),\n",
        "\n",
        "                # 10) Categorical Accordion visibility\n",
        "                gr.update(visible=show_cat),\n",
        "                # 11) Categorical Markdown content\n",
        "                cat_md,\n",
        "                # 12) Categorical Markdown visibility\n",
        "                gr.update(visible=show_cat),\n",
        "                # 13) Categorical bar‐plot\n",
        "                cat_fig,\n",
        "                gr.update(visible=show_cat),\n",
        "\n",
        "                # 14) Text Accordion visibility\n",
        "                gr.update(visible=show_text),\n",
        "                # 15) Text Markdown content\n",
        "                text_md,\n",
        "                # 16) Text Markdown visibility\n",
        "                gr.update(visible=show_text),\n",
        "                # 17) Top‐token bar (text)\n",
        "                tok_fig,\n",
        "                gr.update(visible=show_text),\n",
        "                # 18) Document‐length histogram (text)\n",
        "                len_fig,\n",
        "                gr.update(visible=show_text),\n",
        "                # 19) Sample Real Text\n",
        "                samples_r,\n",
        "                gr.update(visible=show_text and bool(samples_r)),\n",
        "                # 20) Sample Synthetic Text\n",
        "                samples_s,\n",
        "                gr.update(visible=show_text and bool(samples_s)),\n",
        "            )\n",
        "\n",
        "        # Bind the dropdown to all 22 outputs (each paired with a visibility toggle)\n",
        "        self.column_dropdown.change(\n",
        "            show_column_stats,\n",
        "            inputs=[self.report_state, self.column_dropdown],\n",
        "            outputs=[\n",
        "                # Missingness\n",
        "                self.missing_accordion,\n",
        "                self.missing_md,\n",
        "                self.missing_md,\n",
        "\n",
        "                # Numeric\n",
        "                self.numeric_accordion,\n",
        "                self.numeric_md,\n",
        "                self.numeric_md,\n",
        "                self.numeric_cdf_plot,\n",
        "                self.numeric_cdf_plot,\n",
        "                self.numeric_hist_plot,\n",
        "                self.numeric_hist_plot,\n",
        "                self.numeric_kde_plot,\n",
        "                self.numeric_kde_plot,\n",
        "\n",
        "                # Categorical\n",
        "                self.categorical_accordion,\n",
        "                self.categorical_md,\n",
        "                self.categorical_md,\n",
        "                self.categorical_bar_plot,\n",
        "                self.categorical_bar_plot,\n",
        "\n",
        "                # Text\n",
        "                self.text_accordion,\n",
        "                self.text_md,\n",
        "                self.text_md,\n",
        "                self.text_token_plot,\n",
        "                self.text_token_plot,\n",
        "                self.text_len_plot,\n",
        "                self.text_len_plot,\n",
        "                self.text_samples_real,\n",
        "                self.text_samples_real,\n",
        "                self.text_samples_synth,\n",
        "                self.text_samples_synth,\n",
        "            ]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultivariateTab(Tab):\n",
        "    \"\"\"\n",
        "    “PCA Comparison” tab, now extended to include:\n",
        "      1. PCA eigenvector‐cosine metrics\n",
        "      2. Correlation Similarity (numeric × numeric)\n",
        "      3. Contingency Similarity (categorical × categorical)\n",
        "      4. Distinguishability AUC (classifier test)\n",
        "    \"\"\"\n",
        "    def __init__(self, report_state, common_cols_state):\n",
        "        super().__init__(report_state, common_cols_state)\n",
        "\n",
        "    def build_ui(self):\n",
        "        with gr.TabItem(\"Multivariate Analysis\"):\n",
        "            # Slider & button for PCA\n",
        "            self.ncomp_slider = gr.Slider(\n",
        "                minimum=1,\n",
        "                maximum=10,\n",
        "                step=1,\n",
        "                value=2,\n",
        "                label=\"Number of PCA Components to Compare\"\n",
        "            )\n",
        "            self.compute_button = gr.Button(\"Compute PCA & Multivariate Metrics\")\n",
        "\n",
        "            # PCA outputs\n",
        "            self.pca_metrics_output = gr.Markdown(\"\", visible=False)\n",
        "            self.pca_scatter_plot  = gr.Plot(visible=False)\n",
        "\n",
        "            # NEW: Multivariate relationship outputs\n",
        "            self.corr_sim_output        = gr.Markdown(\"\", visible=False, label=\"Correlation Similarity\")\n",
        "            self.contingency_sim_output = gr.Markdown(\"\", visible=False, label=\"Contingency Similarity\")\n",
        "            self.distinguish_auc_output = gr.Markdown(\"\", visible=False, label=\"Distinguishability (ROC AUC)\")\n",
        "\n",
        "    def register_callbacks(self):\n",
        "        # 1) Enable controls once report is ready\n",
        "        def enable_controls(report):\n",
        "            logger.info(\"MultivariateTab: enable_controls called.\")\n",
        "            if report is not None:\n",
        "                logger.info(\"  → Enabling PCA & multivariate controls.\")\n",
        "                return gr.update(interactive=True), gr.update(interactive=True)\n",
        "            else:\n",
        "                logger.info(\"  → Disabling PCA & multivariate controls.\")\n",
        "                return gr.update(interactive=False), gr.update(interactive=False)\n",
        "\n",
        "        self.report_state.change(\n",
        "            enable_controls,\n",
        "            inputs=[self.report_state],\n",
        "            outputs=[self.ncomp_slider, self.compute_button]\n",
        "        )\n",
        "\n",
        "        # 2) When the button is clicked:\n",
        "        def compute_multivariate(report, n_components):\n",
        "            logger.info(f\"MultivariateTab: compute_multivariate called (n_components={n_components}).\")\n",
        "            if report is None:\n",
        "                logger.info(\"  → No report available: clearing all outputs.\")\n",
        "                return (\n",
        "                    \"\",       # pca_metrics_output\n",
        "                    None,     # pca_scatter_plot\n",
        "                    gr.update(visible=False),\n",
        "                    \"\",\n",
        "                    gr.update(visible=False),\n",
        "                    \"\",\n",
        "                    gr.update(visible=False),\n",
        "                    \"\",\n",
        "                    gr.update(visible=False),\n",
        "                )\n",
        "\n",
        "            # --- (A) PCA Eigen‐Cosines (as before) ---\n",
        "            pca_res = report.compute_pca_eigen_cosines(n_components=n_components)\n",
        "            cosines = pca_res['cosine_similarities']\n",
        "            ev_ref  = pca_res['explained_variance_ref']\n",
        "            ev_cmp  = pca_res['explained_variance_cmp']\n",
        "            weighted_cos = pca_res['weighted_cosine']\n",
        "\n",
        "            md_pca = f\"## PCA Eigenvector Cosines (first {len(cosines)} components)\\n\\n\"\n",
        "            for i, cos in enumerate(cosines, start=1):\n",
        "                md_pca += (\n",
        "                    f\"- PC{i} cosine: **{cos:.4f}**  \"\n",
        "                    f\"(exp_var_ref: {ev_ref[i-1]:.4f}, exp_var_cmp: {ev_cmp[i-1]:.4f})\\n\"\n",
        "                )\n",
        "            md_pca += f\"\\n**Weighted Cosine** (weights = ref exp_var_ratio): **{weighted_cos:.4f}**\\n\"\n",
        "            logger.info(\"  → PCA Markdown prepared.\")\n",
        "\n",
        "            scatter_fig = report.plot_pca_scatter(n_components=max(n_components, 2))\n",
        "            show_scatter = len(cosines) >= 2\n",
        "            if show_scatter:\n",
        "                logger.info(\"  → PCA scatter figure prepared.\")\n",
        "            else:\n",
        "                logger.info(\"  → Not enough PCA components for scatter.\")\n",
        "\n",
        "            # --- (B) Correlation Similarity ---\n",
        "            corr_sim = report.compute_correlation_similarity()\n",
        "            if np.isnan(corr_sim):\n",
        "                md_corr = \"Correlation Similarity: **N/A** (need ≥ 2 numeric columns)\"\n",
        "                logger.info(\"  → Correlation similarity: N/A.\")\n",
        "            else:\n",
        "                md_corr = f\"**Correlation Similarity (numeric × numeric)**: **{corr_sim:.4f}**\\n\"\n",
        "                logger.info(f\"  → Correlation similarity = {corr_sim:.6f}\")\n",
        "            show_corr = not np.isnan(corr_sim)\n",
        "\n",
        "            # --- (C) Contingency Similarity ---\n",
        "            cont_sim = report.compute_contingency_similarity()\n",
        "            if np.isnan(cont_sim):\n",
        "                md_cont = \"Contingency Similarity: **N/A** (need ≥ 2 categorical columns)\"\n",
        "                logger.info(\"  → Contingency similarity: N/A.\")\n",
        "            else:\n",
        "                md_cont = f\"**Contingency Similarity (cat × cat)**: **{cont_sim:.4f}**\\n\"\n",
        "                logger.info(f\"  → Contingency similarity = {cont_sim:.6f}\")\n",
        "            show_cont = not np.isnan(cont_sim)\n",
        "\n",
        "            # --- (D) Distinguishability AUC ---\n",
        "            auc = report.compute_distinguishability_auc()\n",
        "            if np.isnan(auc):\n",
        "                md_auc = \"Distinguishability AUC: **N/A** (insufficient mix of numeric/categorical data)\"\n",
        "                logger.info(\"  → Distinguishability AUC: N/A.\")\n",
        "            else:\n",
        "                md_auc = f\"**Distinguishability (ROC AUC)**: **{auc:.4f}**  \\n\"\n",
        "                md_auc += (\n",
        "                    \"- AUC near 0.5 ⇒ synthetic is hard to distinguish from real  \\n\"\n",
        "                    \"- AUC near 1.0 ⇒ classifier easily separates real vs. synthetic\\n\"\n",
        "                )\n",
        "                logger.info(f\"  → Distinguishability AUC = {auc:.6f}\")\n",
        "            show_auc = not np.isnan(auc)\n",
        "\n",
        "            return (\n",
        "                # PCA outputs\n",
        "                md_pca,                # 1: PCA Markdown\n",
        "                scatter_fig,           # 2: PCA scatter\n",
        "                gr.update(visible=True),\n",
        "                gr.update(visible=show_scatter),\n",
        "                # Correlation similarity\n",
        "                md_corr,               # 5: correlation Markdown\n",
        "                gr.update(visible=show_corr),\n",
        "                # Contingency similarity\n",
        "                md_cont,               # 7: contingency Markdown\n",
        "                gr.update(visible=show_cont),\n",
        "                # Distinguishability AUC\n",
        "                md_auc,                # 9: AUC Markdown\n",
        "                gr.update(visible=show_auc),\n",
        "            )\n",
        "\n",
        "        # Bind the button to 10 outputs (5 pairs of (value, visibility))\n",
        "        self.compute_button.click(\n",
        "            compute_multivariate,\n",
        "            inputs=[self.report_state, self.ncomp_slider],\n",
        "            outputs=[\n",
        "                self.pca_metrics_output,      # 1\n",
        "                self.pca_scatter_plot,        # 2\n",
        "                self.pca_metrics_output,      # 3 (visibility)\n",
        "                self.pca_scatter_plot,        # 4\n",
        "                self.corr_sim_output,         # 5\n",
        "                self.corr_sim_output,         # 6 (visibility)\n",
        "                self.contingency_sim_output,  # 7\n",
        "                self.contingency_sim_output,  # 8 (visibility)\n",
        "                self.distinguish_auc_output,  # 9\n",
        "                self.distinguish_auc_output   # 10 (visibility)\n",
        "            ]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GlobalSummaryTab(Tab):\n",
        "    \"\"\"\n",
        "    “Global Summary” tab (Step 5):\n",
        "      - Button to “Compute Global Summary”\n",
        "      - Dataframe: one row per common column, listing all metrics\n",
        "      - Bar‐chart: top-5 most divergent columns by primary_divergence\n",
        "    \"\"\"\n",
        "    def __init__(self, report_state, common_cols_state):\n",
        "        super().__init__(report_state, common_cols_state)\n",
        "\n",
        "    def build_ui(self):\n",
        "        with gr.TabItem(\"Global Summary\"):\n",
        "            self.compute_summary_btn = gr.Button(\"Compute Global Summary\")\n",
        "\n",
        "            # Expanded set of headers to include all metrics + primary_divergence\n",
        "            self.summary_table = gr.Dataframe(\n",
        "                headers=[\n",
        "                    \"column\",\n",
        "                    \"dtype\",\n",
        "\n",
        "                    # Missing-rate (real, syn, Δ)\n",
        "                    \"missing_rate_real\",\n",
        "                    \"missing_rate_syn\",\n",
        "                    \"missing_rate_diff\",\n",
        "\n",
        "                    # Numeric: KS & Wasserstein\n",
        "                    \"ks_similarity\",\n",
        "                    \"wasserstein_raw\",\n",
        "                    \"wasserstein_norm\",\n",
        "\n",
        "                    # Numeric: mean/median/std\n",
        "                    \"mean_real\",\n",
        "                    \"mean_syn\",\n",
        "                    \"mean_diff\",\n",
        "                    \"mean_pct_of_range\",\n",
        "                    \"median_real\",\n",
        "                    \"median_syn\",\n",
        "                    \"median_diff\",\n",
        "                    \"median_pct_of_range\",\n",
        "                    \"std_real\",\n",
        "                    \"std_syn\",\n",
        "                    \"std_diff\",\n",
        "                    \"std_pct_of_std\",\n",
        "\n",
        "                    # Numeric: range coverage\n",
        "                    \"range_coverage\",\n",
        "\n",
        "                    # Categorical: TVD & coverage\n",
        "                    \"tvd_similarity\",\n",
        "                    \"category_coverage\",\n",
        "\n",
        "                    # Text: token-JS, bigram Jaccard/JS\n",
        "                    \"text_tok_js\",\n",
        "                    \"text_bigram_jaccard\",\n",
        "                    \"text_bigram_js\",\n",
        "\n",
        "                    # Text: TF–IDF cosine, vocab Jaccard, OOV rate\n",
        "                    \"text_tfidf_cosine\",\n",
        "                    \"text_vocab_jaccard\",\n",
        "                    \"text_oov_rate\",\n",
        "\n",
        "                    # Text: topic cosine\n",
        "                    \"text_topic_cosine\",\n",
        "\n",
        "                    # Text: length (real, syn, Δ)\n",
        "                    \"len_real\",\n",
        "                    \"len_syn\",\n",
        "                    \"len_diff\",\n",
        "\n",
        "                    # Text: embedding metrics\n",
        "                    \"text_emb_cosine\",\n",
        "                    \"text_emb_mmd\",\n",
        "\n",
        "                    # Global multivariate (same value for every row)\n",
        "                    \"correlation_similarity\",\n",
        "                    \"contingency_similarity\",\n",
        "                    \"distinguishability_auc\",\n",
        "\n",
        "                    # The chosen “primary divergence” for ranking\n",
        "                    \"primary_divergence\"\n",
        "                ],\n",
        "                row_count=\"dynamic\",\n",
        "                interactive=False,\n",
        "                label=\"Per-Column Metrics & Divergence\"\n",
        "            )\n",
        "\n",
        "            # Bar chart for top-5 columns by primary_divergence\n",
        "            self.divergence_bar = gr.Plot(visible=False)\n",
        "\n",
        "    def register_callbacks(self):\n",
        "        # Enable the “Compute” button only once report is loaded\n",
        "        def enable_button(report):\n",
        "            logger.info(\"GlobalSummaryTab: enable_button called.\")\n",
        "            return gr.update(interactive=report is not None)\n",
        "\n",
        "        self.report_state.change(\n",
        "            enable_button,\n",
        "            inputs=[self.report_state],\n",
        "            outputs=[self.compute_summary_btn]\n",
        "        )\n",
        "\n",
        "        # When “Compute Global Summary” is clicked:\n",
        "        def compute_global_summary(report):\n",
        "            logger.info(\"GlobalSummaryTab: compute_global_summary called.\")\n",
        "            if report is None:\n",
        "                logger.info(\"  → No report: clearing table & hiding bar chart.\")\n",
        "                return ([], None, gr.update(visible=False))\n",
        "\n",
        "            rows = []\n",
        "            # 1) Gather univariate metrics per column\n",
        "            for col in report.common_columns:\n",
        "                logger.info(f\"  → Computing univariate for column '{col}'.\")\n",
        "                col_info = report.compute_column(col)\n",
        "                dtype = col_info.pop(\"dtype\")\n",
        "\n",
        "                # Initialize all possible keys to NaN\n",
        "                base_row = {\n",
        "                    \"column\": col,\n",
        "                    \"dtype\": dtype,\n",
        "\n",
        "                    # Missing-rate\n",
        "                    \"missing_rate_real\":  np.nan,\n",
        "                    \"missing_rate_syn\":   np.nan,\n",
        "                    \"missing_rate_diff\":  np.nan,\n",
        "\n",
        "                    # Numeric\n",
        "                    \"ks_similarity\":      np.nan,\n",
        "                    \"wasserstein_raw\":    np.nan,\n",
        "                    \"wasserstein_norm\":   np.nan,\n",
        "                    \"mean_real\":          np.nan,\n",
        "                    \"mean_syn\":           np.nan,\n",
        "                    \"mean_diff\":          np.nan,\n",
        "                    \"mean_pct_of_range\":  np.nan,\n",
        "                    \"median_real\":        np.nan,\n",
        "                    \"median_syn\":         np.nan,\n",
        "                    \"median_diff\":        np.nan,\n",
        "                    \"median_pct_of_range\": np.nan,\n",
        "                    \"std_real\":           np.nan,\n",
        "                    \"std_syn\":            np.nan,\n",
        "                    \"std_diff\":           np.nan,\n",
        "                    \"std_pct_of_std\":     np.nan,\n",
        "                    \"range_coverage\":     np.nan,\n",
        "\n",
        "                    # Categorical\n",
        "                    \"tvd_similarity\":     np.nan,\n",
        "                    \"category_coverage\":  np.nan,\n",
        "\n",
        "                    # Text\n",
        "                    \"text_tok_js\":           np.nan,\n",
        "                    \"text_bigram_jaccard\":   np.nan,\n",
        "                    \"text_bigram_js\":        np.nan,\n",
        "                    \"text_tfidf_cosine\":     np.nan,\n",
        "                    \"text_vocab_jaccard\":    np.nan,\n",
        "                    \"text_oov_rate\":         np.nan,\n",
        "                    \"text_topic_cosine\":     np.nan,\n",
        "                    \"len_real\":              np.nan,\n",
        "                    \"len_syn\":               np.nan,\n",
        "                    \"len_diff\":              np.nan,\n",
        "                    \"text_emb_cosine\":       np.nan,\n",
        "                    \"text_emb_mmd\":          np.nan,\n",
        "\n",
        "                    # Global multivariate (filled below)\n",
        "                    \"correlation_similarity\": np.nan,\n",
        "                    \"contingency_similarity\": np.nan,\n",
        "                    \"distinguishability_auc\": np.nan,\n",
        "\n",
        "                    # To be computed per row\n",
        "                    \"primary_divergence\":    np.nan\n",
        "                }\n",
        "\n",
        "                for key, val in col_info.items():\n",
        "                    if key in base_row:\n",
        "                        base_row[key] = val\n",
        "                rows.append(base_row)\n",
        "\n",
        "            df_metrics = pd.DataFrame(rows)\n",
        "            logger.info(\"  → DataFrame of univariate metrics constructed.\")\n",
        "\n",
        "            # 2) Compute global multivariate metrics once\n",
        "            corr_sim = report.compute_correlation_similarity()\n",
        "            cont_sim = report.compute_contingency_similarity()\n",
        "            auc_sim  = report.compute_distinguishability_auc()\n",
        "            logger.info(f\"  → Multivariate: corr={corr_sim}, cont={cont_sim}, auc={auc_sim}\")\n",
        "\n",
        "            # 3) Broadcast those values to every row\n",
        "            df_metrics[\"correlation_similarity\"]  = corr_sim\n",
        "            df_metrics[\"contingency_similarity\"]  = cont_sim\n",
        "            df_metrics[\"distinguishability_auc\"]  = auc_sim\n",
        "\n",
        "            # 4) Define “primary_divergence” based on dtype\n",
        "            def get_primary_divergence(r):\n",
        "                dtype = r[\"dtype\"]\n",
        "                if dtype == \"numeric\":\n",
        "                    wnorm = r[\"wasserstein_norm\"]\n",
        "                    if not pd.isna(wnorm):\n",
        "                        return wnorm\n",
        "                    ks = r[\"ks_similarity\"]\n",
        "                    return (1.0 - ks) if not pd.isna(ks) else np.nan\n",
        "\n",
        "                elif dtype == \"categorical\":\n",
        "                    tvd = r[\"tvd_similarity\"]\n",
        "                    return (1.0 - tvd) if not pd.isna(tvd) else np.nan\n",
        "\n",
        "                elif dtype == \"text\":\n",
        "                    tfidf = r[\"text_tfidf_cosine\"]\n",
        "                    if not pd.isna(tfidf):\n",
        "                        return (1.0 - tfidf)\n",
        "                    embc = r[\"text_emb_cosine\"]\n",
        "                    return (1.0 - embc) if not pd.isna(embc) else np.nan\n",
        "\n",
        "                else:\n",
        "                    return np.nan\n",
        "\n",
        "            df_metrics[\"primary_divergence\"] = df_metrics.apply(get_primary_divergence, axis=1)\n",
        "            logger.info(\"  → Computed primary_divergence for each column.\")\n",
        "\n",
        "            # 5) Sort by primary_divergence (descending), then take top 5\n",
        "            df_sorted = df_metrics.sort_values(\n",
        "                by=\"primary_divergence\", ascending=False, na_position=\"last\"\n",
        "            )\n",
        "            top_n = min(5, len(df_sorted))\n",
        "            df_top = df_sorted.iloc[:top_n, :]\n",
        "            logger.info(f\"  → Top {top_n} columns by primary_divergence: {df_top['column'].tolist()}\")\n",
        "\n",
        "            # 6) Build bar chart for top_n columns\n",
        "            fig = go.Figure()\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=df_top[\"column\"],\n",
        "                    y=df_top[\"primary_divergence\"],\n",
        "                    text=df_top[\"primary_divergence\"].round(4),\n",
        "                    textposition=\"auto\",\n",
        "                    name=\"Primary Divergence\"\n",
        "                )\n",
        "            )\n",
        "            fig.update_layout(\n",
        "                title=f\"Top {top_n} Divergent Columns (by Primary Divergence)\",\n",
        "                xaxis_title=\"Column\",\n",
        "                yaxis_title=\"Divergence\",\n",
        "                margin=dict(l=40, r=40, t=50, b=40)\n",
        "            )\n",
        "            logger.info(\"  → Bar chart for top divergences ready.\")\n",
        "\n",
        "            # 7) Return the full table and show the bar chart\n",
        "            return (\n",
        "                df_metrics.values.tolist(),\n",
        "                fig,\n",
        "                gr.update(visible=True)\n",
        "            )\n",
        "\n",
        "        self.compute_summary_btn.click(\n",
        "            compute_global_summary,\n",
        "            inputs=[self.report_state],\n",
        "            outputs=[\n",
        "                self.summary_table,   # table data (list of lists)\n",
        "                self.divergence_bar,  # bar chart figure\n",
        "                self.divergence_bar   # bar chart visibility toggle\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "zBgGI-92ISlP",
        "outputId": "30545286-cd03-4664-e28d-b44bc06408da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6ed079be3f3bef6124.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://6ed079be3f3bef6124.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d32197b955f5131ecf.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://6ffb363646dfcc34a0.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://6d806f1c19404b72ec.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://6ed079be3f3bef6124.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ─── “Build & Launch” with Three Tabs ─────────────────────────────────\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    logger.info(\"Launching Gradio Blocks app with three tabs.\")\n",
        "    # (1) Create the two State objects inside the Blocks context\n",
        "    report_state      = gr.State(None)\n",
        "    common_cols_state = gr.State(None)\n",
        "\n",
        "    # (2) Instantiate all three tabs, passing the shared states\n",
        "    general_tab  = GeneralStatsTab(report_state, common_cols_state)\n",
        "    multivar_tab = MultivariateTab(report_state, common_cols_state)\n",
        "    global_tab   = GlobalSummaryTab(report_state, common_cols_state)\n",
        "\n",
        "    # (3) Build the UI for all three tabs under *one* Tabs container\n",
        "    with gr.Tabs():\n",
        "        general_tab.build_ui()\n",
        "        multivar_tab.build_ui()\n",
        "        global_tab.build_ui()\n",
        "\n",
        "    # (4) Register callbacks for each tab (after the Tabs block)\n",
        "    general_tab.register_callbacks()\n",
        "    multivar_tab.register_callbacks()\n",
        "    global_tab.register_callbacks()\n",
        "\n",
        "# (5) Launch the app (in Colab use share=True)\n",
        "demo.launch(share=True, debug=True)  # or debug=True if you want logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uwud-NfBtKUp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
